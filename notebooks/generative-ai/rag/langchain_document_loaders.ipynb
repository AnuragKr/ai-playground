{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjyiD8I5r7jB"
      },
      "source": [
        "# Exploring Document Loaders in LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1KvMtf54l0d"
      },
      "source": [
        "## Install OpenAI, HuggingFace and LangChain dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSu-YKYjNs0F",
        "outputId": "9f32509f-9169-4e52-8c48-43a6f0f744cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.3.11\n",
            "  Downloading langchain-0.3.11-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.11) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.11) (2.0.41)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.11) (3.11.15)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.24 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.11) (0.3.60)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.11) (0.3.8)\n",
            "Collecting langsmith<0.3,>=0.1.17 (from langchain==0.3.11)\n",
            "  Downloading langsmith-0.2.11-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting numpy<2,>=1.22.4 (from langchain==0.3.11)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.11) (2.11.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.11) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.11) (9.1.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.11) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.11) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.11) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.11) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.11) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.11) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.11) (1.20.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain==0.3.11) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain==0.3.11) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain==0.3.11) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.17->langchain==0.3.11) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.17->langchain==0.3.11) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.17->langchain==0.3.11) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.11) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.11) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.11) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.11) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.11) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.11) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.11) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.11) (3.2.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain==0.3.11) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain==0.3.11) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain==0.3.11) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.24->langchain==0.3.11) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain==0.3.11) (1.3.1)\n",
            "Downloading langchain-0.3.11-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.2.11-py3-none-any.whl (326 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.9/326.9 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, langsmith, langchain\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.3.42\n",
            "    Uninstalling langsmith-0.3.42:\n",
            "      Successfully uninstalled langsmith-0.3.42\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.25\n",
            "    Uninstalling langchain-0.3.25:\n",
            "      Successfully uninstalled langchain-0.3.25\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-0.3.11 langsmith-0.2.11 numpy-1.26.4\n",
            "Collecting langchain-openai==0.2.12\n",
            "  Downloading langchain_openai-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.2.12) (0.3.60)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.55.3 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.2.12) (1.81.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.2.12) (0.9.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.126 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (0.2.11)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (4.13.2)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (2.11.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.2.12) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.2.12) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.2.12) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.2.12) (2.4.0)\n",
            "Downloading langchain_openai-0.2.12-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-openai\n",
            "Successfully installed langchain-openai-0.2.12\n",
            "Collecting langchain-community==0.3.11\n",
            "  Downloading langchain_community-0.3.11-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.11) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.11) (2.0.41)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.11) (3.11.15)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community==0.3.11)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community==0.3.11)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.11 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.11) (0.3.11)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.24 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.11) (0.3.60)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.11) (0.2.11)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.11) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community==0.3.11)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.11) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.11) (9.1.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (1.20.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.11)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.11)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (2.11.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.0.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.11)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.11) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.11) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.11) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.11) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.11) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.3.11) (3.2.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.11)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.3.1)\n",
            "Downloading langchain_community-0.3.11-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.11 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain==0.3.11\n",
        "!pip install langchain-openai==0.2.12\n",
        "!pip install langchain-community==0.3.11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CB6lHzbz5a10",
        "outputId": "52b86ab3-2890-4bab-ab7b-b4cfcd581415"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unstructured==0.14.0 in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (0.14.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (5.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (3.9.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (4.13.4)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (2.14.1)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (0.6.7)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (2025.2.18)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (1.0.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (1.26.4)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (3.13.0)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (4.13.2)\n",
            "Requirement already satisfied: unstructured-client in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (0.36.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (1.17.2)\n",
            "Requirement already satisfied: pikepdf in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (9.8.1)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (20250327)\n",
            "Requirement already satisfied: python-pptx<=0.6.23 in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (0.6.23)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (3.4.2)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (1.17.0)\n",
            "Requirement already satisfied: pypandoc in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (1.15)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (1.18.0)\n",
            "Requirement already satisfied: unstructured.pytesseract>=0.3.12 in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (0.3.15)\n",
            "Requirement already satisfied: google-cloud-vision in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (3.10.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (1.1.2)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (4.2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (2.2.2)\n",
            "Requirement already satisfied: pillow-heif in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (0.22.0)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (3.8)\n",
            "Requirement already satisfied: msg-parser in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (1.2.0)\n",
            "Requirement already satisfied: unstructured-inference==0.7.31 in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (0.7.31)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (3.1.5)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (2.0.1)\n",
            "Requirement already satisfied: layoutparser[layoutmodels,tesseract] in /usr/local/lib/python3.11/dist-packages (from unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (0.3.4)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.11/dist-packages (from unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (0.0.20)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (0.31.4)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.11/dist-packages (from unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (4.11.0.86)\n",
            "Requirement already satisfied: onnxruntime>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (1.22.0)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (4.52.2)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.11/dist-packages (from python-pptx<=0.6.23->unstructured[all-docs]==0.14.0) (11.2.1)\n",
            "Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from python-pptx<=0.6.23->unstructured[all-docs]==0.14.0) (3.2.3)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from unstructured.pytesseract>=0.3.12->unstructured[all-docs]==0.14.0) (24.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (2.7)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (0.9.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[all-docs]==0.14.0) (2.24.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-vision->unstructured[all-docs]==0.14.0) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-vision->unstructured[all-docs]==0.14.0) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-vision->unstructured[all-docs]==0.14.0) (5.29.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (1.17.0)\n",
            "Requirement already satisfied: olefile>=0.46 in /usr/local/lib/python3.11/dist-packages (from msg-parser->unstructured[all-docs]==0.14.0) (0.47)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (4.67.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl->unstructured[all-docs]==0.14.0) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->unstructured[all-docs]==0.14.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->unstructured[all-docs]==0.14.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->unstructured[all-docs]==0.14.0) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six->unstructured[all-docs]==0.14.0) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six->unstructured[all-docs]==0.14.0) (43.0.3)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.11/dist-packages (from pikepdf->unstructured[all-docs]==0.14.0) (1.2.18)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (2025.4.26)\n",
            "Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (24.1.0)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (1.6.0)\n",
            "Requirement already satisfied: pydantic>=2.11.2 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (2.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (1.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[all-docs]==0.14.0) (1.17.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[all-docs]==0.14.0) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[all-docs]==0.14.0) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[all-docs]==0.14.0) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[all-docs]==0.14.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[all-docs]==0.14.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[all-docs]==0.14.0) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (0.16.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (25.2.10)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (1.13.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (3.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (2025.3.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (1.1.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (1.15.3)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.11/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (0.1.10)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (0.11.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (0.21.0+cu124)\n",
            "Requirement already satisfied: effdet in /usr/local/lib/python3.11/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (0.4.1)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (0.3.13)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[all-docs]==0.14.0) (2.22)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[all-docs]==0.14.0) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.17.0->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (10.0)\n",
            "Requirement already satisfied: timm>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (1.0.15)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (2.0.8)\n",
            "Requirement already satisfied: omegaconf>=2.0 in /usr/local/lib/python3.11/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (2.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.17.0->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (1.3.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from iopath->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (3.1.1)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (4.30.1)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf>=2.0->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (4.9.3)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (3.10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (3.2.3)\n"
          ]
        }
      ],
      "source": [
        "# takes 2 - 5 mins to install on Colab\n",
        "!pip install \"unstructured[all-docs]==0.14.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhEW-tOywUgt",
        "outputId": "0c49f28a-00eb-45c3-f231-00a1794e8672"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 697 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.8 [186 kB]\n",
            "Fetched 186 kB in 48s (3,888 B/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 126109 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.8_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.8) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.8) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "# install OCR dependencies for unstructured\n",
        "!sudo apt-get install tesseract-ocr\n",
        "!sudo apt-get install poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWNjOhSbRaOw",
        "outputId": "3be1d622-95e0-4b9e-8c59-e45db100df41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jq==1.7.0 in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Requirement already satisfied: pypdf==4.2.0 in /usr/local/lib/python3.11/dist-packages (4.2.0)\n",
            "Requirement already satisfied: pymupdf==1.24.4 in /usr/local/lib/python3.11/dist-packages (1.24.4)\n",
            "Requirement already satisfied: PyMuPDFb==1.24.3 in /usr/local/lib/python3.11/dist-packages (from pymupdf==1.24.4) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install jq==1.7.0\n",
        "!pip install pypdf==4.2.0\n",
        "!pip install pymupdf==1.24.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqX0BkkWZ_e0"
      },
      "source": [
        "## Document Loaders\n",
        "\n",
        "Document loaders are used to import data from various sources into LangChain as `Document` objects. A `Document` typically includes a piece of text along with its associated metadata.\n",
        "\n",
        "### Examples of Document Loaders:\n",
        "\n",
        "- **Text File Loader:** Loads data from a simple `.txt` file.\n",
        "- **Web Page Loader:** Retrieves the text content from any web page.\n",
        "- **YouTube Video Transcript Loader:** Loads transcripts from YouTube videos.\n",
        "\n",
        "### Functionality:\n",
        "\n",
        "- **Load Method:** Each document loader has a `load` method that enables the loading of data as documents from a pre-configured source.\n",
        "- **Lazy Load Option:** Some loaders also support a \"lazy load\" feature, which allows data to be loaded into memory gradually as needed.\n",
        "\n",
        "For more detailed information, visit [LangChain's document loader documentation](https://python.langchain.com/docs/modules/data_connection/document_loaders/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEx_nNkHLqZY"
      },
      "source": [
        "### Text Loader\n",
        "\n",
        "The simplest loader reads in a file as text and places it all into one document.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Al7y4r93LKA4",
        "outputId": "9f3e188e-d896-4cbd-af38-55a27c764cde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  5169  100  5169    0     0  25327      0 --:--:-- --:--:-- --:--:-- 25463\n"
          ]
        }
      ],
      "source": [
        "!curl -o README.md https://raw.githubusercontent.com/langchain-ai/langchain/master/README.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3ehpI19eLKEo"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(\"./README.md\")\n",
        "doc = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxR60glzvzQN",
        "outputId": "b56f64d0-ab29-4eff-f8b1-204d6a68baf1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "len(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "Uu-7i1PcdHpE",
        "outputId": "0dab0707-de83-44cb-df5a-22485fb4140c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.documents.base.Document"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.documents.base.Document</b><br/>def __init__(page_content: str, **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/langchain_core/documents/base.py</a>Class for storing a piece of text and associated metadata.\n",
              "\n",
              "Example:\n",
              "\n",
              "    .. code-block:: python\n",
              "\n",
              "        from langchain_core.documents import Document\n",
              "\n",
              "        document = Document(\n",
              "            page_content=&quot;Hello, world!&quot;,\n",
              "            metadata={&quot;source&quot;: &quot;https://example.com&quot;}\n",
              "        )</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 255);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "type(doc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFbJx7Q9LKG4",
        "outputId": "a785f548-8f6c-4efe-a720-d566bba7470d",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<picture>\n",
            "  <source media=\"(prefers-color-scheme: light)\" srcset=\"docs/static/img/logo-dark.svg\">\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "print(doc[0].page_content[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTf3za4x7CtK"
      },
      "source": [
        "### Markdown Loader\n",
        "\n",
        "Markdown is a lightweight markup language for creating formatted text using a plain-text editor.\n",
        "\n",
        "This showcases how to load Markdown documents into a langchain document format that we can use in our pipelines and chains."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEmzUQK9_640"
      },
      "source": [
        "Load the whole document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJt1cdnFOeP0"
      },
      "source": [
        "Download nltk packages if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CD-IBE4POUwu",
        "outputId": "8350c994-69c5-4c85-c946-a2a15c1517c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "D1uZEc-f8TyV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50c2c114-13b9-42fa-8a30-cd581e17dc2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
        "\n",
        "loader = UnstructuredMarkdownLoader(\"./README.md\", mode='single')\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPkNbhXv7Z3B",
        "outputId": "4cf4a43f-e417-4b1a-b0d2-9d2dec60e869"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "m547b2uc8vy9",
        "outputId": "58982962-8d32-4d5e-e17c-7074c761bc96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.documents.base.Document"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.documents.base.Document</b><br/>def __init__(page_content: str, **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/langchain_core/documents/base.py</a>Class for storing a piece of text and associated metadata.\n",
              "\n",
              "Example:\n",
              "\n",
              "    .. code-block:: python\n",
              "\n",
              "        from langchain_core.documents import Document\n",
              "\n",
              "        document = Document(\n",
              "            page_content=&quot;Hello, world!&quot;,\n",
              "            metadata={&quot;source&quot;: &quot;https://example.com&quot;}\n",
              "        )</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 255);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "type(docs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gia_n-T8Ytn",
        "outputId": "06a2627c-e6f5-49ef-c0b2-b2731fe2b870"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[!NOTE]\n",
            "Looking for the JS/TS library? Check out LangChain.js.\n",
            "\n",
            "LangChain is a framework for buildin\n"
          ]
        }
      ],
      "source": [
        "print(docs[0].page_content[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SOPDX8w85MeG"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
        "\n",
        "loader = UnstructuredMarkdownLoader(\"./README.md\", mode=\"elements\")\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dr4WVaEg-qTr",
        "outputId": "a9272885-173a-487a-d4b6-43e24f6e3974"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpfQwXmD-rji",
        "outputId": "c654a81b-dd69-4e9b-8d66-5e2a776ce2b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': './README.md', 'last_modified': '2025-05-29T11:31:12', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md', 'category': 'NarrativeText', 'element_id': 'ada4984f55e3bfe7057f8abd1b24a809'}, page_content='[!NOTE]\\nLooking for the JS/TS library? Check out LangChain.js.'),\n",
              " Document(metadata={'source': './README.md', 'last_modified': '2025-05-29T11:31:12', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md', 'category': 'NarrativeText', 'element_id': 'd9dd2676fd3ef14eba932c9da5c5636b'}, page_content='LangChain is a framework for building LLM-powered applications. It helps you chain\\ntogether interoperable components and third-party integrations to simplify AI\\napplication development —  all while future-proofing decisions as the underlying\\ntechnology evolves.'),\n",
              " Document(metadata={'source': './README.md', 'last_modified': '2025-05-29T11:31:12', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md', 'category': 'Title', 'element_id': 'd096b8fd4a734bd4645dd41583152f12'}, page_content='bash\\npip install -U langchain'),\n",
              " Document(metadata={'source': './README.md', 'last_modified': '2025-05-29T11:31:12', 'languages': ['eng'], 'parent_id': 'd096b8fd4a734bd4645dd41583152f12', 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md', 'category': 'NarrativeText', 'element_id': 'b9bc6693902b26077316240c50b8f2c2'}, page_content='To learn more about LangChain, check out\\nthe docs. If you’re looking for more\\nadvanced customization or agent orchestration, check out\\nLangGraph, our framework for building\\ncontrollable agent workflows.'),\n",
              " Document(metadata={'source': './README.md', 'last_modified': '2025-05-29T11:31:12', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md', 'category': 'Title', 'element_id': '6c4906b888bacd02e86fcd62eecb7c4a'}, page_content='Why use LangChain?'),\n",
              " Document(metadata={'source': './README.md', 'last_modified': '2025-05-29T11:31:12', 'languages': ['eng'], 'parent_id': '6c4906b888bacd02e86fcd62eecb7c4a', 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md', 'category': 'NarrativeText', 'element_id': '9dfecbf1c43eadad822524820c29b732'}, page_content='LangChain helps developers build applications powered by LLMs through a standard\\ninterface for models, embeddings, vector stores, and more.'),\n",
              " Document(metadata={'source': './README.md', 'last_modified': '2025-05-29T11:31:12', 'languages': ['eng'], 'parent_id': '6c4906b888bacd02e86fcd62eecb7c4a', 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md', 'category': 'NarrativeText', 'element_id': '08aa38139038f600d56be5799cd79770'}, page_content='Use LangChain for:\\n- Real-time data augmentation. Easily connect LLMs to diverse data sources and\\nexternal / internal systems, drawing from LangChain’s vast library of integrations with\\nmodel providers, tools, vector stores, retrievers, and more.\\n- Model interoperability. Swap models in and out as your engineering team\\nexperiments to find the best choice for your application’s needs. As the industry\\nfrontier evolves, adapt quickly — LangChain’s abstractions keep you moving without\\nlosing momentum.'),\n",
              " Document(metadata={'source': './README.md', 'last_modified': '2025-05-29T11:31:12', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md', 'category': 'Title', 'element_id': '8c79a11e4939fe6cc8a0a62187715af6'}, page_content='LangChain’s ecosystem'),\n",
              " Document(metadata={'source': './README.md', 'last_modified': '2025-05-29T11:31:12', 'languages': ['eng'], 'parent_id': '8c79a11e4939fe6cc8a0a62187715af6', 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md', 'category': 'NarrativeText', 'element_id': 'c8ea6b63778256f1ab89b8db63334008'}, page_content='While the LangChain framework can be used standalone, it also integrates seamlessly\\nwith any LangChain product, giving developers a full suite of tools when building LLM\\napplications.'),\n",
              " Document(metadata={'source': './README.md', 'last_modified': '2025-05-29T11:31:12', 'languages': ['eng'], 'parent_id': '8c79a11e4939fe6cc8a0a62187715af6', 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md', 'category': 'NarrativeText', 'element_id': '47d44e641923360b7dc95ee8c4541cd8'}, page_content='To improve your LLM application development, pair LangChain with:')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "docs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUcCM72S-vHb",
        "outputId": "76f59088-0a97-4c55-fd3d-8913dcbe543c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'NarrativeText': 7, 'Title': 4, 'ListItem': 7})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "from collections import Counter\n",
        "Counter([doc.metadata['category'] for doc in docs])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h8Hb5sKABXD"
      },
      "source": [
        "Comparing Unstructured.io loaders vs LangChain wrapper API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "RZqKGM8q8RpS"
      },
      "outputs": [],
      "source": [
        "from unstructured.partition.md import partition_md\n",
        "\n",
        "docs = partition_md(filename=\"./README.md\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQES4WJY80IM",
        "outputId": "90bce9b0-05fa-44e4-db2c-393c8e7fe149"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88F0QIA2-_96",
        "outputId": "43957f20-4ad4-4d0e-bb4d-ccfba16237bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<unstructured.documents.elements.NarrativeText at 0x78bc92b9c790>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x78bc8c57a310>,\n",
              " <unstructured.documents.elements.Title at 0x78bc8c579ed0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x78bc8c57ab90>,\n",
              " <unstructured.documents.elements.Title at 0x78bc8c57ac50>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x78bc8c57add0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x78bc8c57ae50>,\n",
              " <unstructured.documents.elements.Title at 0x78bc8c57af10>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x78bc8c57afd0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x78bc8c57b050>]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "docs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVGTYLJf7fgC",
        "outputId": "e5db7e0d-1e17-4d0b-a413-c721dbd2a8fc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'type': 'NarrativeText',\n",
              " 'element_id': 'ada4984f55e3bfe7057f8abd1b24a809',\n",
              " 'text': '[!NOTE]\\nLooking for the JS/TS library? Check out LangChain.js.',\n",
              " 'metadata': {'last_modified': '2025-05-29T11:31:12',\n",
              "  'languages': ['eng'],\n",
              "  'filetype': 'text/markdown',\n",
              "  'file_directory': '.',\n",
              "  'filename': 'README.md'}}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "docs[0].to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDMICcgV_GiF",
        "outputId": "3ec74161-90f6-40c4-bd93-aa37cd5a643f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'type': 'NarrativeText',\n",
              " 'element_id': 'd9dd2676fd3ef14eba932c9da5c5636b',\n",
              " 'text': 'LangChain is a framework for building LLM-powered applications. It helps you chain\\ntogether interoperable components and third-party integrations to simplify AI\\napplication development —  all while future-proofing decisions as the underlying\\ntechnology evolves.',\n",
              " 'metadata': {'last_modified': '2025-05-29T11:31:12',\n",
              "  'languages': ['eng'],\n",
              "  'filetype': 'text/markdown',\n",
              "  'file_directory': '.',\n",
              "  'filename': 'README.md'}}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "docs[1].to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g87ZcUNG_Ka_",
        "outputId": "986c8fb2-1361-4bc8-8350-2d344ce89e4d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'last_modified': '2025-05-29T11:31:12', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md'}, page_content='[!NOTE]\\nLooking for the JS/TS library? Check out LangChain.js.'),\n",
              " Document(metadata={'last_modified': '2025-05-29T11:31:12', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md'}, page_content='LangChain is a framework for building LLM-powered applications. It helps you chain\\ntogether interoperable components and third-party integrations to simplify AI\\napplication development —  all while future-proofing decisions as the underlying\\ntechnology evolves.'),\n",
              " Document(metadata={'last_modified': '2025-05-29T11:31:12', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md'}, page_content='bash\\npip install -U langchain'),\n",
              " Document(metadata={'last_modified': '2025-05-29T11:31:12', 'languages': ['eng'], 'parent_id': 'd096b8fd4a734bd4645dd41583152f12', 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md'}, page_content='To learn more about LangChain, check out\\nthe docs. If you’re looking for more\\nadvanced customization or agent orchestration, check out\\nLangGraph, our framework for building\\ncontrollable agent workflows.'),\n",
              " Document(metadata={'last_modified': '2025-05-29T11:31:12', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md'}, page_content='Why use LangChain?'),\n",
              " Document(metadata={'last_modified': '2025-05-29T11:31:12', 'languages': ['eng'], 'parent_id': '6c4906b888bacd02e86fcd62eecb7c4a', 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md'}, page_content='LangChain helps developers build applications powered by LLMs through a standard\\ninterface for models, embeddings, vector stores, and more.'),\n",
              " Document(metadata={'last_modified': '2025-05-29T11:31:12', 'languages': ['eng'], 'parent_id': '6c4906b888bacd02e86fcd62eecb7c4a', 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md'}, page_content='Use LangChain for:\\n- Real-time data augmentation. Easily connect LLMs to diverse data sources and\\nexternal / internal systems, drawing from LangChain’s vast library of integrations with\\nmodel providers, tools, vector stores, retrievers, and more.\\n- Model interoperability. Swap models in and out as your engineering team\\nexperiments to find the best choice for your application’s needs. As the industry\\nfrontier evolves, adapt quickly — LangChain’s abstractions keep you moving without\\nlosing momentum.'),\n",
              " Document(metadata={'last_modified': '2025-05-29T11:31:12', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md'}, page_content='LangChain’s ecosystem'),\n",
              " Document(metadata={'last_modified': '2025-05-29T11:31:12', 'languages': ['eng'], 'parent_id': '8c79a11e4939fe6cc8a0a62187715af6', 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md'}, page_content='While the LangChain framework can be used standalone, it also integrates seamlessly\\nwith any LangChain product, giving developers a full suite of tools when building LLM\\napplications.'),\n",
              " Document(metadata={'last_modified': '2025-05-29T11:31:12', 'languages': ['eng'], 'parent_id': '8c79a11e4939fe6cc8a0a62187715af6', 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md'}, page_content='To improve your LLM application development, pair LangChain with:')]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "lc_docs = [Document(page_content=doc.text,\n",
        "                    metadata=doc.metadata.to_dict())\n",
        "              for doc in docs]\n",
        "lc_docs[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSSBO_Y3fB8P"
      },
      "source": [
        "### CSV Loader\n",
        "\n",
        "A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas.\n",
        "\n",
        "LangChain implements a CSV Loader that will load CSV files into a sequence of `Document` objects. Each row of the CSV file is converted to one document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "QuoCrfQUODLt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame with some dummy real estate data\n",
        "data = {\n",
        "    'Property_ID': [101, 102, 103, 104, 105],\n",
        "    'Address': ['123 Elm St', '456 Oak St', '789 Pine St', '321 Maple St', '654 Cedar St'],\n",
        "    'City': ['Springfield', 'Rivertown', 'Laketown', 'Hillside', 'Sunnyvale'],\n",
        "    'State': ['CA', 'TX', 'FL', 'NY', 'CO'],\n",
        "    'Zip_Code': [98765, 87654, 76543, 65432, 54321],\n",
        "    'Bedrooms': [3, 2, 4, 3, 5],\n",
        "    'Bathrooms': [2, 1, 3, 2, 4],\n",
        "    'Listing_Price': [500000, 350000, 600000, 475000, 750000]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "hG-_cJFULKI3"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "\n",
        "loader = CSVLoader(file_path=\"./data.csv\")\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dgrc_HhzLKMF",
        "outputId": "9a9c6a6f-a661-4458-f84a-6750ec38ba4f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': './data.csv', 'row': 0}, page_content='Property_ID: 101\\nAddress: 123 Elm St\\nCity: Springfield\\nState: CA\\nZip_Code: 98765\\nBedrooms: 3\\nBathrooms: 2\\nListing_Price: 500000'),\n",
              " Document(metadata={'source': './data.csv', 'row': 1}, page_content='Property_ID: 102\\nAddress: 456 Oak St\\nCity: Rivertown\\nState: TX\\nZip_Code: 87654\\nBedrooms: 2\\nBathrooms: 1\\nListing_Price: 350000'),\n",
              " Document(metadata={'source': './data.csv', 'row': 2}, page_content='Property_ID: 103\\nAddress: 789 Pine St\\nCity: Laketown\\nState: FL\\nZip_Code: 76543\\nBedrooms: 4\\nBathrooms: 3\\nListing_Price: 600000'),\n",
              " Document(metadata={'source': './data.csv', 'row': 3}, page_content='Property_ID: 104\\nAddress: 321 Maple St\\nCity: Hillside\\nState: NY\\nZip_Code: 65432\\nBedrooms: 3\\nBathrooms: 2\\nListing_Price: 475000'),\n",
              " Document(metadata={'source': './data.csv', 'row': 4}, page_content='Property_ID: 105\\nAddress: 654 Cedar St\\nCity: Sunnyvale\\nState: CO\\nZip_Code: 54321\\nBedrooms: 5\\nBathrooms: 4\\nListing_Price: 750000')]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8vVzwmseaoH",
        "outputId": "f27fa59b-a3d9-4826-da30-27a578ae1a42"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': './data.csv', 'row': 0}, page_content='Property_ID: 101\\nAddress: 123 Elm St\\nCity: Springfield\\nState: CA\\nZip_Code: 98765\\nBedrooms: 3\\nBathrooms: 2\\nListing_Price: 500000')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "docs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdENGJXQOXA-",
        "outputId": "236c7ead-0e1e-42b9-9a5b-803392d44f78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Property_ID: 101\n",
            "Address: 123 Elm St\n",
            "City: Springfield\n",
            "State: CA\n",
            "Zip_Code: 98765\n",
            "Bedrooms: 3\n",
            "Bathrooms: 2\n",
            "Listing_Price: 500000\n"
          ]
        }
      ],
      "source": [
        "print(docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HVFIEiiB1Wu"
      },
      "source": [
        "`CSVLoader` will accept a `csv_args` kwarg that supports customization of arguments passed to Python's csv.`DictReader`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "XxLiCAl2CHE2"
      },
      "outputs": [],
      "source": [
        "loader = CSVLoader(file_path=\"./data.csv\",\n",
        "                   csv_args={\n",
        "                      \"delimiter\": \",\",\n",
        "                      \"quotechar\": '\"',\n",
        "                      \"fieldnames\": [\"Property ID\", \"Address\", \"City\", \"State\",\n",
        "                                     \"Zip Code\", \"Bedrooms\", \"Bathrooms\", \"Price\"],\n",
        "                   },\n",
        "                  )\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WuekvnIDShM",
        "outputId": "3f79e9ca-5e9a-436e-9b2e-2fe750031768"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': './data.csv', 'row': 0}, page_content='Property ID: Property_ID\\nAddress: Address\\nCity: City\\nState: State\\nZip Code: Zip_Code\\nBedrooms: Bedrooms\\nBathrooms: Bathrooms\\nPrice: Listing_Price'),\n",
              " Document(metadata={'source': './data.csv', 'row': 1}, page_content='Property ID: 101\\nAddress: 123 Elm St\\nCity: Springfield\\nState: CA\\nZip Code: 98765\\nBedrooms: 3\\nBathrooms: 2\\nPrice: 500000'),\n",
              " Document(metadata={'source': './data.csv', 'row': 2}, page_content='Property ID: 102\\nAddress: 456 Oak St\\nCity: Rivertown\\nState: TX\\nZip Code: 87654\\nBedrooms: 2\\nBathrooms: 1\\nPrice: 350000'),\n",
              " Document(metadata={'source': './data.csv', 'row': 3}, page_content='Property ID: 103\\nAddress: 789 Pine St\\nCity: Laketown\\nState: FL\\nZip Code: 76543\\nBedrooms: 4\\nBathrooms: 3\\nPrice: 600000'),\n",
              " Document(metadata={'source': './data.csv', 'row': 4}, page_content='Property ID: 104\\nAddress: 321 Maple St\\nCity: Hillside\\nState: NY\\nZip Code: 65432\\nBedrooms: 3\\nBathrooms: 2\\nPrice: 475000'),\n",
              " Document(metadata={'source': './data.csv', 'row': 5}, page_content='Property ID: 105\\nAddress: 654 Cedar St\\nCity: Sunnyvale\\nState: CO\\nZip Code: 54321\\nBedrooms: 5\\nBathrooms: 4\\nPrice: 750000')]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXseBM98BgkG"
      },
      "source": [
        "Unstructured.io loads the entire CSV as a single table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "89SAdDs7A4eR"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import UnstructuredCSVLoader\n",
        "\n",
        "loader = UnstructuredCSVLoader(\"./data.csv\")\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvic6kW8BEn3",
        "outputId": "aa1815dc-37ef-478c-b49c-84cd064ca9a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54gFzsugBnad",
        "outputId": "24d29041-f463-4f18-a79f-4a575ff5ea5a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': './data.csv'}, page_content='\\n\\n\\nProperty_ID\\nAddress\\nCity\\nState\\nZip_Code\\nBedrooms\\nBathrooms\\nListing_Price\\n\\n\\n101\\n123 Elm St\\nSpringfield\\nCA\\n98765\\n3\\n2\\n500000\\n\\n\\n102\\n456 Oak St\\nRivertown\\nTX\\n87654\\n2\\n1\\n350000\\n\\n\\n103\\n789 Pine St\\nLaketown\\nFL\\n76543\\n4\\n3\\n600000\\n\\n\\n104\\n321 Maple St\\nHillside\\nNY\\n65432\\n3\\n2\\n475000\\n\\n\\n105\\n654 Cedar St\\nSunnyvale\\nCO\\n54321\\n5\\n4\\n750000\\n\\n\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "docs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrHu4ts4fHgH"
      },
      "source": [
        "### JSON Loader\n",
        "\n",
        "[JSON (JavaScript Object Notation)](https://en.wikipedia.org/wiki/JSON) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute–value pairs and arrays (or other serializable values).\n",
        "\n",
        "[JSON Lines](https://jsonlines.org/) is a file format where each line is a valid JSON value.\n",
        "\n",
        "LangChain implements a [JSONLoader](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.json_loader.JSONLoader.html) to convert JSON and JSONL data into LangChain `Document` objects. It uses a specified [`jq` schema](https://en.wikipedia.org/wiki/Jq_(programming_language)) to parse the JSON files, allowing for the extraction of specific fields into the content and metadata of the LangChain Document.\n",
        "\n",
        "It uses the `jq` python package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ALFBPgsnOd0L"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Sample data dictionary similar to the one you provided but with modified contents\n",
        "data = {\n",
        "    'image': {'creation_timestamp': 1675549016, 'uri': 'image_of_the_meeting.jpg'},\n",
        "    'is_still_participant': True,\n",
        "    'joinable_mode': {'link': '', 'mode': 1},\n",
        "    'magic_words': [],\n",
        "    'messages': [\n",
        "        {'content': 'See you soon!',\n",
        "         'sender_name': 'User B',\n",
        "         'timestamp_ms': 1675597571851},\n",
        "        {'content': 'Thanks for the update! See you then.',\n",
        "         'sender_name': 'User A',\n",
        "         'timestamp_ms': 1675597435669},\n",
        "        {'content': 'Actually, the green one is sold out.',\n",
        "         'sender_name': 'User B',\n",
        "         'timestamp_ms': 1675596277579},\n",
        "        {'content': 'I was hoping to purchase the green one!',\n",
        "         'sender_name': 'User A',\n",
        "         'timestamp_ms': 1675595140251},\n",
        "        {'content': 'I’m really interested in the green one, not the red!',\n",
        "         'sender_name': 'User A',\n",
        "         'timestamp_ms': 1675595109305},\n",
        "        {'content': 'Here’s the $150 for it.',\n",
        "         'sender_name': 'User B',\n",
        "         'timestamp_ms': 1675595068468},\n",
        "        {'photos': [{'creation_timestamp': 1675595059,\n",
        "                     'uri': 'image_of_the_item.jpg'}],\n",
        "         'sender_name': 'User B',\n",
        "         'timestamp_ms': 1675595060730},\n",
        "        {'content': 'It typically sells for at least $200 online',\n",
        "         'sender_name': 'User B',\n",
        "         'timestamp_ms': 1675595045152},\n",
        "        {'content': 'How much are you asking?',\n",
        "         'sender_name': 'User A',\n",
        "         'timestamp_ms': 1675594799696},\n",
        "        {'content': 'Good morning! $50 is far too low.',\n",
        "         'sender_name': 'User B',\n",
        "         'timestamp_ms': 1675577876645},\n",
        "        {'content': 'Hello! I’m interested in the item you posted. I can offer $50. Let me know if that works for you. Thanks!',\n",
        "         'sender_name': 'User A',\n",
        "         'timestamp_ms': 1675549022673}\n",
        "    ],\n",
        "    'participants': [{'name': 'User A'}, {'name': 'User B'}],\n",
        "    'thread_path': 'inbox/User A and User B chat',\n",
        "    'title': 'User A and User B chat'\n",
        "}\n",
        "\n",
        "# Save the modified data to a JSON file\n",
        "with open('chat_data.json', 'w') as file:\n",
        "    json.dump(data, file, indent=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ9YwY4xG7KD"
      },
      "source": [
        "To load the full data as a single document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "aD1kjnQRQ0h5"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import JSONLoader\n",
        "\n",
        "loader = JSONLoader(\n",
        "    file_path='./chat_data.json',\n",
        "    jq_schema='.',\n",
        "    text_content=False)\n",
        "\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQIAvS8rGpNx",
        "outputId": "7c154b08-ff01-44a5-8399-5fab498e0d4e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPmHC3aYRgFA",
        "outputId": "b7f18f8b-7514-43cd-939d-1bf55e1705c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/chat_data.json', 'seq_num': 1}, page_content='{\"image\": {\"creation_timestamp\": 1675549016, \"uri\": \"image_of_the_meeting.jpg\"}, \"is_still_participant\": true, \"joinable_mode\": {\"link\": \"\", \"mode\": 1}, \"magic_words\": [], \"messages\": [{\"content\": \"See you soon!\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675597571851}, {\"content\": \"Thanks for the update! See you then.\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675597435669}, {\"content\": \"Actually, the green one is sold out.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675596277579}, {\"content\": \"I was hoping to purchase the green one!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675595140251}, {\"content\": \"I\\\\u2019m really interested in the green one, not the red!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675595109305}, {\"content\": \"Here\\\\u2019s the $150 for it.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675595068468}, {\"photos\": [{\"creation_timestamp\": 1675595059, \"uri\": \"image_of_the_item.jpg\"}], \"sender_name\": \"User B\", \"timestamp_ms\": 1675595060730}, {\"content\": \"It typically sells for at least $200 online\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675595045152}, {\"content\": \"How much are you asking?\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675594799696}, {\"content\": \"Good morning! $50 is far too low.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675577876645}, {\"content\": \"Hello! I\\\\u2019m interested in the item you posted. I can offer $50. Let me know if that works for you. Thanks!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675549022673}], \"participants\": [{\"name\": \"User A\"}, {\"name\": \"User B\"}], \"thread_path\": \"inbox/User A and User B chat\", \"title\": \"User A and User B chat\"}')]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY48CWHhG_KF"
      },
      "source": [
        "Suppose we are interested in extracting the values under the `messages` key of the JSON data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VC5cWidHSFuR",
        "outputId": "1ed5bcad-017f-4391-e482-6a9b46a2d143"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/chat_data.json', 'seq_num': 1}, page_content='{\"content\": \"See you soon!\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675597571851}'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 2}, page_content='{\"content\": \"Thanks for the update! See you then.\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675597435669}'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 3}, page_content='{\"content\": \"Actually, the green one is sold out.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675596277579}'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 4}, page_content='{\"content\": \"I was hoping to purchase the green one!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675595140251}'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 5}, page_content='{\"content\": \"I\\\\u2019m really interested in the green one, not the red!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675595109305}'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 6}, page_content='{\"content\": \"Here\\\\u2019s the $150 for it.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675595068468}'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 7}, page_content='{\"photos\": [{\"creation_timestamp\": 1675595059, \"uri\": \"image_of_the_item.jpg\"}], \"sender_name\": \"User B\", \"timestamp_ms\": 1675595060730}'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 8}, page_content='{\"content\": \"It typically sells for at least $200 online\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675595045152}'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 9}, page_content='{\"content\": \"How much are you asking?\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675594799696}'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 10}, page_content='{\"content\": \"Good morning! $50 is far too low.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675577876645}'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 11}, page_content='{\"content\": \"Hello! I\\\\u2019m interested in the item you posted. I can offer $50. Let me know if that works for you. Thanks!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675549022673}')]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "loader = JSONLoader(\n",
        "    file_path='./chat_data.json',\n",
        "    jq_schema='.messages[]',\n",
        "    text_content=False)\n",
        "\n",
        "data = loader.load()\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK59yCK3H-C-"
      },
      "source": [
        "Suppose we are interested in extracting the values under the `content` field within the `messages` key of the JSON data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7Bq2FHlSVb7",
        "outputId": "5b2fd256-94da-4320-ce08-bbd22f30aa4b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/chat_data.json', 'seq_num': 1}, page_content='See you soon!'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 2}, page_content='Thanks for the update! See you then.'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 3}, page_content='Actually, the green one is sold out.'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 4}, page_content='I was hoping to purchase the green one!'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 5}, page_content='I’m really interested in the green one, not the red!'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 6}, page_content='Here’s the $150 for it.'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 7}, page_content=''),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 8}, page_content='It typically sells for at least $200 online'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 9}, page_content='How much are you asking?'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 10}, page_content='Good morning! $50 is far too low.'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 11}, page_content='Hello! I’m interested in the item you posted. I can offer $50. Let me know if that works for you. Thanks!')]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "loader = JSONLoader(\n",
        "    file_path='./chat_data.json',\n",
        "    jq_schema='.messages[].content',\n",
        "    text_content=False)\n",
        "\n",
        "data = loader.load()\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZB2fxI9fKxC"
      },
      "source": [
        "### PDF Loaders\n",
        "\n",
        "[Portable Document Format (PDF)](https://en.wikipedia.org/wiki/PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.\n",
        "\n",
        "LangChain integrates with a host of PDF parsers. Some are simple and relatively low-level; others will support OCR and image-processing, or perform advanced document layout analysis. The right choice will depend on your use-case and through experimentation.\n",
        "\n",
        "Here we will see how to load PDF documents into the LangChain `Document` format\n",
        "\n",
        "We download a research paper to experiment with"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2NWiC51KDbm"
      },
      "source": [
        "If the following command fails you can download the paper manually by going to http://arxiv.org/pdf/2103.15348.pdf, save it as `layoutparser_paper.pdf`and upload it on the left in Colab from the upload files option"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_zMe1cES7Tb",
        "outputId": "1d8055d8-e622-4b1e-f362-721b32a2b63d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-29 11:52:20--  http://arxiv.org/pdf/2103.15348.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.67.42, 151.101.195.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2103.15348 [following]\n",
            "--2025-05-29 11:52:20--  http://arxiv.org/pdf/2103.15348\n",
            "Reusing existing connection to arxiv.org:80.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4686220 (4.5M) [application/pdf]\n",
            "Saving to: ‘layoutparser_paper.pdf’\n",
            "\n",
            "\rlayoutparser_paper.   0%[                    ]       0  --.-KB/s               \rlayoutparser_paper. 100%[===================>]   4.47M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2025-05-29 11:52:20 (53.3 MB/s) - ‘layoutparser_paper.pdf’ saved [4686220/4686220]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O 'layoutparser_paper.pdf' 'http://arxiv.org/pdf/2103.15348.pdf'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qs1ZBxbxfNXq"
      },
      "source": [
        "#### PyPDFLoader\n",
        "\n",
        "Here we load a PDF using `pypdf` into list of documents, where each document contains the page content and metadata with page number. Typically each PDF page becomes one document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0be_BLqTI0H",
        "outputId": "e0834b77-9e2b-48d4-cbce-1c3953027a05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pypdf/_crypt_providers/_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from this module in 48.0.0.\n",
            "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"./layoutparser_paper.pdf\")\n",
        "pages = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJZo2Hk9hMgb",
        "outputId": "8319321a-d77e-4437-c914-c6e7fa6625c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "len(pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrqsL5lhToUY",
        "outputId": "10ad7029-9cf7-4868-e478-9c7682f5aca3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': './layoutparser_paper.pdf', 'page': 0}, page_content='LayoutParser : A Uniﬁed Toolkit for Deep\\nLearning Based Document Image Analysis\\nZejiang Shen1( \\x00), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\\nLee4, Jacob Carlson3, and Weining Li5\\n1Allen Institute for AI\\nshannons@allenai.org\\n2Brown University\\nruochen zhang@brown.edu\\n3Harvard University\\n{melissadell,jacob carlson }@fas.harvard.edu\\n4University of Washington\\nbcgl@cs.washington.edu\\n5University of Waterloo\\nw422li@uwaterloo.ca\\nAbstract. Recent advances in document image analysis (DIA) have been\\nprimarily driven by the application of neural networks. Ideally, research\\noutcomes could be easily deployed in production and extended for further\\ninvestigation. However, various factors like loosely organized codebases\\nand sophisticated model conﬁgurations complicate the easy reuse of im-\\nportant innovations by a wide audience. Though there have been on-going\\neﬀorts to improve reusability and simplify deep learning (DL) model\\ndevelopment in disciplines like natural language processing and computer\\nvision, none of them are optimized for challenges in the domain of DIA.\\nThis represents a major gap in the existing toolkit, as DIA is central to\\nacademic research across a wide range of disciplines in the social sciences\\nand humanities. This paper introduces LayoutParser , an open-source\\nlibrary for streamlining the usage of DL in DIA research and applica-\\ntions. The core LayoutParser library comes with a set of simple and\\nintuitive interfaces for applying and customizing DL models for layout de-\\ntection, character recognition, and many other document processing tasks.\\nTo promote extensibility, LayoutParser also incorporates a community\\nplatform for sharing both pre-trained models and full document digiti-\\nzation pipelines. We demonstrate that LayoutParser is helpful for both\\nlightweight and large-scale digitization pipelines in real-word use cases.\\nThe library is publicly available at https://layout-parser.github.io .\\nKeywords: Document Image Analysis ·Deep Learning ·Layout Analysis\\n·Character Recognition ·Open Source library ·Toolkit.\\n1 Introduction\\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of\\ndocument image analysis (DIA) tasks including document image classiﬁcation [ 11,arXiv:2103.15348v2  [cs.CV]  21 Jun 2021')"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "pages[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xU5qyqJT7TB",
        "outputId": "52f5d3aa-0f02-4306-8d12-74c413214e7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LayoutParser : A Uniﬁed Toolkit for Deep\n",
            "Learning Based Document Image Analysis\n",
            "Zejiang Shen1( \u0000), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\n",
            "Lee4, Jacob Carlson3, and Weining Li5\n",
            "1Allen Institute for AI\n",
            "shannons@allenai.org\n",
            "2Brown University\n",
            "ruochen zhang@brown.edu\n",
            "3Harvard University\n",
            "{melissadell,jacob carlson }@fas.harvard.edu\n",
            "4University of Washington\n",
            "bcgl@cs.washington.edu\n",
            "5University of Waterloo\n",
            "w422li@uwaterloo.ca\n",
            "Abstract. Recent advances in document image analysis (DIA) have been\n",
            "primarily driven by the application of neural networks. Ideally, research\n",
            "outcomes could be easily deployed in production and extended for further\n",
            "investigation. However, various factors like loosely organized codebases\n",
            "and sophisticated model conﬁgurations complicate the easy reuse of im-\n",
            "portant innovations by a wide audience. Though there have been on-going\n",
            "eﬀorts to improve reusability and simplify deep learning (DL) model\n",
            "development in disciplines like natural language processing and computer\n",
            "vision, none of them are optimized for challenges in the domain of DIA.\n",
            "This represents a major gap in the existing toolkit, as DIA is central to\n",
            "academic research across a wide range of disciplines in the social sciences\n",
            "and humanities. This paper introduces LayoutParser , an open-source\n",
            "library for streamlining the usage of DL in DIA research and applica-\n",
            "tions. The core LayoutParser library comes with a set of simple and\n",
            "intuitive interfaces for applying and customizing DL models for layout de-\n",
            "tection, character recognition, and many other document processing tasks.\n",
            "To promote extensibility, LayoutParser also incorporates a community\n",
            "platform for sharing both pre-trained models and full document digiti-\n",
            "zation pipelines. We demonstrate that LayoutParser is helpful for both\n",
            "lightweight and large-scale digitization pipelines in real-word use cases.\n",
            "The library is publicly available at https://layout-parser.github.io .\n",
            "Keywords: Document Image Analysis ·Deep Learning ·Layout Analysis\n",
            "·Character Recognition ·Open Source library ·Toolkit.\n",
            "1 Introduction\n",
            "Deep Learning(DL)-based approaches are the state-of-the-art for a wide range of\n",
            "document image analysis (DIA) tasks including document image classiﬁcation [ 11,arXiv:2103.15348v2  [cs.CV]  21 Jun 2021\n"
          ]
        }
      ],
      "source": [
        "print(pages[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcDMgTfBTrZL",
        "outputId": "6c757798-1962-4592-a735-0d7da42c84a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LayoutParser : A Uniﬁed Toolkit for DL-Based DIA 5\n",
            "Table 1: Current layout detection models in the LayoutParser model zoo\n",
            "Dataset Base Model1Large Model Notes\n",
            "PubLayNet [38] F / M M Layouts of modern scientiﬁc documents\n",
            "PRImA [3] M - Layouts of scanned modern magazines and scientiﬁc reports\n",
            "Newspaper [17] F - Layouts of scanned US newspapers from the 20th century\n",
            "TableBank [18] F F Table region on modern scientiﬁc and business document\n",
            "HJDataset [31] F / M - Layouts of history Japanese documents\n",
            "1For each dataset, we train several models of diﬀerent sizes for diﬀerent needs (the trade-oﬀ between accuracy\n",
            "vs. computational cost). For “base model” and “large model”, we refer to using the ResNet 50 or ResNet 101\n",
            "backbones [ 13], respectively. One can train models of diﬀerent architectures, like Faster R-CNN [ 28] (F) and Mask\n",
            "R-CNN [ 12] (M). For example, an F in the Large Model column indicates it has a Faster R-CNN model trained\n",
            "using the ResNet 101 backbone. The platform is maintained and a number of additions will be made to the model\n",
            "zoo in coming months.\n",
            "layout data structures , which are optimized for eﬃciency and versatility. 3) When\n",
            "necessary, users can employ existing or customized OCR models via the uniﬁed\n",
            "API provided in the OCR module . 4)LayoutParser comes with a set of utility\n",
            "functions for the visualization and storage of the layout data. 5) LayoutParser\n",
            "is also highly customizable, via its integration with functions for layout data\n",
            "annotation and model training . We now provide detailed descriptions for each\n",
            "component.\n",
            "3.1 Layout Detection Models\n",
            "InLayoutParser , a layout model takes a document image as an input and\n",
            "generates a list of rectangular boxes for the target content regions. Diﬀerent\n",
            "from traditional methods, it relies on deep convolutional neural networks rather\n",
            "than manually curated rules to identify content regions. It is formulated as an\n",
            "object detection problem and state-of-the-art models like Faster R-CNN [ 28] and\n",
            "Mask R-CNN [ 12] are used. This yields prediction results of high accuracy and\n",
            "makes it possible to build a concise, generalized interface for layout detection.\n",
            "LayoutParser , built upon Detectron2 [ 35], provides a minimal API that can\n",
            "perform layout detection with only four lines of code in Python:\n",
            "1import layoutparser as lp\n",
            "2image = cv2. imread (\" image_file \") # load images\n",
            "3model = lp. Detectron2LayoutModel (\n",
            "4 \"lp :// PubLayNet / faster_rcnn_R_50_FPN_3x / config \")\n",
            "5layout = model . detect ( image )\n",
            "LayoutParser provides a wealth of pre-trained model weights using various\n",
            "datasets covering diﬀerent languages, time periods, and document types. Due to\n",
            "domain shift [ 7], the prediction performance can notably drop when models are ap-\n",
            "plied to target samples that are signiﬁcantly diﬀerent from the training dataset. As\n",
            "document structures and layouts vary greatly in diﬀerent domains, it is important\n",
            "to select models trained on a dataset similar to the test samples. A semantic syntax\n",
            "is used for initializing the model weights in LayoutParser , using both the dataset\n",
            "name and model name lp://<dataset-name>/<model-architecture-name> .\n"
          ]
        }
      ],
      "source": [
        "print(pages[4].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_T0_7KtfUJj"
      },
      "source": [
        "#### PyMuPDFLoader\n",
        "\n",
        "This is the fastest of the PDF parsing options, and contains detailed metadata about the PDF and its pages, as well as returns one document per page. It uses the `pymupdf` library internally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "l3KTMV_3XmfL"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "loader = PyMuPDFLoader(\"./layoutparser_paper.pdf\")\n",
        "pages = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXUAxQnJhuHO",
        "outputId": "d595c124-a8b4-480f-d349-4c24bda0a903"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "len(pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZM5poERdRpL",
        "outputId": "2d7eeb0c-a78f-48a3-9de1-05d36ed15a67"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': './layoutparser_paper.pdf', 'file_path': './layoutparser_paper.pdf', 'page': 0, 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210622012710Z', 'modDate': 'D:20210622012710Z', 'trapped': ''}, page_content='LayoutParser: A Uniﬁed Toolkit for Deep\\nLearning Based Document Image Analysis\\nZejiang Shen1 (\\x00), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\\nLee4, Jacob Carlson3, and Weining Li5\\n1 Allen Institute for AI\\nshannons@allenai.org\\n2 Brown University\\nruochen zhang@brown.edu\\n3 Harvard University\\n{melissadell,jacob carlson}@fas.harvard.edu\\n4 University of Washington\\nbcgl@cs.washington.edu\\n5 University of Waterloo\\nw422li@uwaterloo.ca\\nAbstract. Recent advances in document image analysis (DIA) have been\\nprimarily driven by the application of neural networks. Ideally, research\\noutcomes could be easily deployed in production and extended for further\\ninvestigation. However, various factors like loosely organized codebases\\nand sophisticated model conﬁgurations complicate the easy reuse of im-\\nportant innovations by a wide audience. Though there have been on-going\\neﬀorts to improve reusability and simplify deep learning (DL) model\\ndevelopment in disciplines like natural language processing and computer\\nvision, none of them are optimized for challenges in the domain of DIA.\\nThis represents a major gap in the existing toolkit, as DIA is central to\\nacademic research across a wide range of disciplines in the social sciences\\nand humanities. This paper introduces LayoutParser, an open-source\\nlibrary for streamlining the usage of DL in DIA research and applica-\\ntions. The core LayoutParser library comes with a set of simple and\\nintuitive interfaces for applying and customizing DL models for layout de-\\ntection, character recognition, and many other document processing tasks.\\nTo promote extensibility, LayoutParser also incorporates a community\\nplatform for sharing both pre-trained models and full document digiti-\\nzation pipelines. We demonstrate that LayoutParser is helpful for both\\nlightweight and large-scale digitization pipelines in real-word use cases.\\nThe library is publicly available at https://layout-parser.github.io.\\nKeywords: Document Image Analysis · Deep Learning · Layout Analysis\\n· Character Recognition · Open Source library · Toolkit.\\n1\\nIntroduction\\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of\\ndocument image analysis (DIA) tasks including document image classiﬁcation [11,\\narXiv:2103.15348v2  [cs.CV]  21 Jun 2021\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "pages[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uhr0Y1C90TH0",
        "outputId": "c87cef3a-6ff3-4eee-b3f6-8544c397fa26"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'source': './layoutparser_paper.pdf',\n",
              " 'file_path': './layoutparser_paper.pdf',\n",
              " 'page': 0,\n",
              " 'total_pages': 16,\n",
              " 'format': 'PDF 1.5',\n",
              " 'title': '',\n",
              " 'author': '',\n",
              " 'subject': '',\n",
              " 'keywords': '',\n",
              " 'creator': 'LaTeX with hyperref',\n",
              " 'producer': 'pdfTeX-1.40.21',\n",
              " 'creationDate': 'D:20210622012710Z',\n",
              " 'modDate': 'D:20210622012710Z',\n",
              " 'trapped': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "pages[0].metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdaA9hkHXmhs",
        "outputId": "1bdf439c-f238-4344-f4cb-f6d82be3da2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LayoutParser: A Uniﬁed Toolkit for Deep\n",
            "Learning Based Document Image Analysis\n",
            "Zejiang Shen1 (\u0000), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\n",
            "Lee4, Jacob Carlson3, and Weining Li5\n",
            "1 Allen Institute for AI\n",
            "shannons@allenai.org\n",
            "2 Brown University\n",
            "ruochen zhang@brown.edu\n",
            "3 Harvard University\n",
            "{melissadell,jacob carlson}@fas.harvard.edu\n",
            "4 University of Washington\n",
            "bcgl@cs.washington.edu\n",
            "5 University of Waterloo\n",
            "w422li@uwaterloo.ca\n",
            "Abstract. Recent advances in document image analysis (DIA) have been\n",
            "primarily driven by the application of neural networks. Ideally, research\n",
            "outcomes could be easily deployed in production and extended for further\n",
            "investigation. However, various factors like loosely organized codebases\n",
            "and sophisticated model conﬁgurations complicate the easy reuse of im-\n",
            "portant innovations by a wide audience. Though there have been on-going\n",
            "eﬀorts to improve reusability and simplify deep learning (DL) model\n",
            "development in disciplines like natural language processing and computer\n",
            "vision, none of them are optimized for challenges in the domain of DIA.\n",
            "This represents a major gap in the existing toolkit, as DIA is central to\n",
            "academic research across a wide range of disciplines in the social sciences\n",
            "and humanities. This paper introduces LayoutParser, an open-source\n",
            "library for streamlining the usage of DL in DIA research and applica-\n",
            "tions. The core LayoutParser library comes with a set of simple and\n",
            "intuitive interfaces for applying and customizing DL models for layout de-\n",
            "tection, character recognition, and many other document processing tasks.\n",
            "To promote extensibility, LayoutParser also incorporates a community\n",
            "platform for sharing both pre-trained models and full document digiti-\n",
            "zation pipelines. We demonstrate that LayoutParser is helpful for both\n",
            "lightweight and large-scale digitization pipelines in real-word use cases.\n",
            "The library is publicly available at https://layout-parser.github.io.\n",
            "Keywords: Document Image Analysis · Deep Learning · Layout Analysis\n",
            "· Character Recognition · Open Source library · Toolkit.\n",
            "1\n",
            "Introduction\n",
            "Deep Learning(DL)-based approaches are the state-of-the-art for a wide range of\n",
            "document image analysis (DIA) tasks including document image classiﬁcation [11,\n",
            "arXiv:2103.15348v2  [cs.CV]  21 Jun 2021\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(pages[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDxTzEe0Le6I",
        "outputId": "1efdb4c2-4ff0-4ccd-94c8-5452bbfb0e13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LayoutParser: A Uniﬁed Toolkit for DL-Based DIA\n",
            "5\n",
            "Table 1: Current layout detection models in the LayoutParser model zoo\n",
            "Dataset\n",
            "Base Model1 Large Model\n",
            "Notes\n",
            "PubLayNet [38]\n",
            "F / M\n",
            "M\n",
            "Layouts of modern scientiﬁc documents\n",
            "PRImA [3]\n",
            "M\n",
            "-\n",
            "Layouts of scanned modern magazines and scientiﬁc reports\n",
            "Newspaper [17]\n",
            "F\n",
            "-\n",
            "Layouts of scanned US newspapers from the 20th century\n",
            "TableBank [18]\n",
            "F\n",
            "F\n",
            "Table region on modern scientiﬁc and business document\n",
            "HJDataset [31]\n",
            "F / M\n",
            "-\n",
            "Layouts of history Japanese documents\n",
            "1 For each dataset, we train several models of diﬀerent sizes for diﬀerent needs (the trade-oﬀbetween accuracy\n",
            "vs. computational cost). For “base model” and “large model”, we refer to using the ResNet 50 or ResNet 101\n",
            "backbones [13], respectively. One can train models of diﬀerent architectures, like Faster R-CNN [28] (F) and Mask\n",
            "R-CNN [12] (M). For example, an F in the Large Model column indicates it has a Faster R-CNN model trained\n",
            "using the ResNet 101 backbone. The platform is maintained and a number of additions will be made to the model\n",
            "zoo in coming months.\n",
            "layout data structures, which are optimized for eﬃciency and versatility. 3) When\n",
            "necessary, users can employ existing or customized OCR models via the uniﬁed\n",
            "API provided in the OCR module. 4) LayoutParser comes with a set of utility\n",
            "functions for the visualization and storage of the layout data. 5) LayoutParser\n",
            "is also highly customizable, via its integration with functions for layout data\n",
            "annotation and model training. We now provide detailed descriptions for each\n",
            "component.\n",
            "3.1\n",
            "Layout Detection Models\n",
            "In LayoutParser, a layout model takes a document image as an input and\n",
            "generates a list of rectangular boxes for the target content regions. Diﬀerent\n",
            "from traditional methods, it relies on deep convolutional neural networks rather\n",
            "than manually curated rules to identify content regions. It is formulated as an\n",
            "object detection problem and state-of-the-art models like Faster R-CNN [28] and\n",
            "Mask R-CNN [12] are used. This yields prediction results of high accuracy and\n",
            "makes it possible to build a concise, generalized interface for layout detection.\n",
            "LayoutParser, built upon Detectron2 [35], provides a minimal API that can\n",
            "perform layout detection with only four lines of code in Python:\n",
            "1 import\n",
            "layoutparser as lp\n",
            "2 image = cv2.imread(\"image_file\") # load\n",
            "images\n",
            "3 model = lp. Detectron2LayoutModel (\n",
            "4\n",
            "\"lp:// PubLayNet/ faster_rcnn_R_50_FPN_3x /config\")\n",
            "5 layout = model.detect(image)\n",
            "LayoutParser provides a wealth of pre-trained model weights using various\n",
            "datasets covering diﬀerent languages, time periods, and document types. Due to\n",
            "domain shift [7], the prediction performance can notably drop when models are ap-\n",
            "plied to target samples that are signiﬁcantly diﬀerent from the training dataset. As\n",
            "document structures and layouts vary greatly in diﬀerent domains, it is important\n",
            "to select models trained on a dataset similar to the test samples. A semantic syntax\n",
            "is used for initializing the model weights in LayoutParser, using both the dataset\n",
            "name and model name lp://<dataset-name>/<model-architecture-name>.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(pages[4].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zvyk9ACL8fx"
      },
      "source": [
        "#### UnstructuredPDFLoader\n",
        "\n",
        "[Unstructured.io](https://unstructured-io.github.io/unstructured/) supports a common interface for working with unstructured or semi-structured file formats, such as Markdown or PDF. LangChain's [`UnstructuredPDFLoader`](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.pdf.UnstructuredPDFLoader.html) integrates with Unstructured to parse PDF documents into LangChain [`Document`](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html) objects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv4PBKyfR2od"
      },
      "source": [
        "Load PDF as a single document - no complex parsing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install pdfminer.six with a compatible version\n",
        "!pip install pdfminer.six==20221105"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "PzbkP0UR5bVY",
        "outputId": "d9aabfd3-1730-4291-d37e-ac89259beb07"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six==20221105\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20221105) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20221105) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20221105) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105) (2.22)\n",
            "Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pdfminer.six\n",
            "  Attempting uninstall: pdfminer.six\n",
            "    Found existing installation: pdfminer.six 20250327\n",
            "    Uninstalling pdfminer.six-20250327:\n",
            "      Successfully uninstalled pdfminer.six-20250327\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pdfplumber 0.11.6 requires pdfminer.six==20250327, but you have pdfminer-six 20221105 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pdfminer.six-20221105\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pdfminer"
                ]
              },
              "id": "9c25d5964c56419ca299c63f639aacef"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Z8qz1HKWMRTf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9101a51-20d4-42c2-a8fc-5488b4ffad7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pypdf/_crypt_providers/_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from this module in 48.0.0.\n",
            "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
        "\n",
        "loader = UnstructuredPDFLoader('./layoutparser_paper.pdf')\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDWTGeMFMbI7",
        "outputId": "02cea62a-1d5d-4cd7-a874-54dd52fa1738"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUmEA9qfMdIc",
        "outputId": "8e916c3b-625f-47ad-eece-276c091b3c25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 2 0 2\n",
            "\n",
            "n u J\n",
            "\n",
            "1 2\n",
            "\n",
            "]\n",
            "\n",
            "V C . s c [\n",
            "\n",
            "2 v 8 4 3 5 1 . 3 0 1 2 : v i X r a\n",
            "\n",
            "LayoutParser: A Uniﬁed Toolkit for Deep Learning Based Document Image Analysis\n",
            "\n",
            "Zejiang Shen1 ((cid:0)), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain Lee4, Jacob Carlson3, and Weining Li5\n",
            "\n",
            "1 Allen Institute for AI shannons@allenai.org 2 Brown University ruochen zhang@brown.edu 3 Harvard University {melissadell,jacob carlson}@fas.harvard.edu 4 University of Washington bcgl@cs.washington.edu 5 University of Waterloo w422li@uwaterloo.ca\n",
            "\n",
            "Abstract. Recent advances in document image analysis (DIA) have been primarily driven by the application of neural networks. Ideally, research outcomes could be easily deployed in production and extended for further investigation. However, various factors like loosely organized codebases and sophisticated model conﬁgurations complicate the easy reuse of im- portant innovations by a wide audience. Though there have been on-going eﬀorts to improve reusability and simplify d\n"
          ]
        }
      ],
      "source": [
        "print(data[0].page_content[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeB_kHTjR6u5"
      },
      "source": [
        "Load PDF with complex parsing, table detection and chunking by sections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252,
          "referenced_widgets": [
            "a0bdf462f1644153b5c798268af9311c",
            "3371035b0a8d4fe28bf49d98aa2f3a88",
            "117e63248e10471d839261faa99eea04",
            "a19a4a39337d48fbb74e46331f182a77",
            "5eba90e93f0e47229e2379e47b032358",
            "15c4feebb07b4b45b94dc9c40a4c7b2d",
            "0c8f2c7239074fe4b5d44ad1f761e796",
            "1eb45ece889347bf95476f04fdad392c",
            "945d5eff4ffd4290ad28d806961a3797",
            "48f14f4cc68d405e9dc7c307219a8bd7",
            "a9c844c92a694cc1bbbf999e26ee21d4",
            "d4e80d70186a4545ad05f3fba1bd193b",
            "f2ae4a96cbdf4170885482b8b6567d56",
            "283786ae92814994b20f693c985fc0cd",
            "bfd6723afa4246c2b75352a9ceb62dfc",
            "3518f3f1394349b89aeefeeb64a240f9",
            "552216d666254e8c9a713880b75aa526",
            "78bfb1f3370f4720a7917f437c4841c5",
            "a6af5bd232f04b1f9d2d36f431dce328",
            "f42767541924485abd58f6157c294af7",
            "39e417f90aac486e8f77eeb7c985ab46",
            "24eed9a6c7d54b39b2d514df57195f8a",
            "41256336abd74ce489b211456af4e90d",
            "96fede9942dc450ba1fdaabc6b912257",
            "55138bba571b48288ae03f29b1b5112e",
            "dd7c6a1a8c4e4c3a93e9e49c28734006",
            "a79aba0edd8d40b5a93ade2ebb813b56",
            "f0c9bd2809a04493ac4f2f9c9e33a2ff",
            "cddc6f1bec414e68ba1ca17f0851a14c",
            "6dd76543dd4942639111000843ee1a38",
            "d1f6d4bd712c4b0e811ab00883b114e3",
            "e1ce222bef1546738b361789116e8599",
            "378f1acbae6b4c5793e6d347b62f0330",
            "c1f023ff52a74f868c9f43a771327eca",
            "b67ed462a32a45fabf5ff3c1382c164e",
            "d6016b9615f24fde940a792da0fa23be",
            "c4bdaebfdf9146d7a8990d80b60d5ccb",
            "ae089cdcb18b4be5842eaf90660dcaaa",
            "dccaef03f0294f258eb1111a01143a68",
            "af3ea2fdf0234b67ba28d98519a6eb7c",
            "5da13d72dbdf473a982b9c4c4649cf0d",
            "51453d81251c4209bdcac4ae00564c0f",
            "2b05479cc2ec430485e97961878686d5",
            "b69894e2c5834c6ea3bb0f77162dc06c"
          ]
        },
        "id": "jbH6ZJLcNs0s",
        "outputId": "3cf5e1a6-172c-4bc8-a1b2-f4bf11041d95"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "yolox_l0.05.onnx:   0%|          | 0.00/217M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0bdf462f1644153b5c798268af9311c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4e80d70186a4545ad05f3fba1bd193b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/115M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41256336abd74ce489b211456af4e90d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/46.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1f023ff52a74f868c9f43a771327eca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
            "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# takes 3-4 mins on Colab\n",
        "loader = UnstructuredPDFLoader('./layoutparser_paper.pdf',\n",
        "                               strategy='hi_res',\n",
        "                               extract_images_in_pdf=False,\n",
        "                               infer_table_structure=True,\n",
        "                               chunking_strategy=\"by_title\",\n",
        "                               max_characters=4000, # max size of chunks\n",
        "                               new_after_n_chars=3800, # preferred size of chunks\n",
        "                               combine_text_under_n_chars=2000, # smaller chunks < 2000 chars will be combined into a larger chunk\n",
        "                               mode='elements')\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WvtzXCgMots",
        "outputId": "073779bb-0017-499a-9d22-7e79f878138e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JM5fCpJNPPiv",
        "outputId": "0c1bb2f2-9750-4ed9-aa94-f315e40639aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['CompositeElement',\n",
              " 'CompositeElement',\n",
              " 'CompositeElement',\n",
              " 'CompositeElement',\n",
              " 'CompositeElement',\n",
              " 'Table',\n",
              " 'CompositeElement',\n",
              " 'CompositeElement',\n",
              " 'CompositeElement',\n",
              " 'Table',\n",
              " 'CompositeElement',\n",
              " 'CompositeElement',\n",
              " 'CompositeElement',\n",
              " 'CompositeElement',\n",
              " 'CompositeElement',\n",
              " 'CompositeElement',\n",
              " 'CompositeElement',\n",
              " 'CompositeElement']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "[doc.metadata['category'] for doc in data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f7zcYUxMq0t",
        "outputId": "5e092fa1-903d-45f9-b004-2a186c90c5a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': './layoutparser_paper.pdf', 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2023-01-23T09:15:33', 'page_number': 1, 'orig_elements': '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', 'file_directory': '.', 'filename': 'layoutparser_paper.pdf', 'category': 'CompositeElement', 'element_id': 'a9d856e62be9762f29e02db1df238cf5'}, page_content='1 2 0 2\\n\\nn u J 1 2 ] V C . s c [\\n\\n2 v 8 4 3 5 1 . 3 0 1 2 : v i X r a\\n\\nLayoutParser: A Uniﬁed Toolkit for Deep Learning Based Document Image Analysis\\n\\nZejiang Shen! (4), Ruochen Zhang”, Melissa Dell?, Benjamin Charles Germain Lee*, Jacob Carlson’, and Weining Li®\\n\\n1 Allen Institute for AI shannons@allenai.org 2 Brown University ruochen zhang@brown.edu 3 Harvard University {melissadell,jacob carlson}@fas.harvard.edu 4 University of Washington bcgl@cs.washington.edu 5 University of Waterloo w422li@uwaterloo.ca\\n\\nAbstract. Recent advances in document image analysis (DIA) have been primarily driven by the application of neural networks. Ideally, research outcomes could be easily deployed in production and extended for further investigation. However, various factors like loosely organized codebases and sophisticated model conﬁgurations complicate the easy reuse of im- portant innovations by a wide audience. Though there have been on-going eﬀorts to improve reusability and simplify deep learning (DL) model development in disciplines like natural language processing and computer vision, none of them are optimized for challenges in the domain of DIA. This represents a major gap in the existing toolkit, as DIA is central to academic research across a wide range of disciplines in the social sciences and humanities. This paper introduces LayoutParser, an open-source library for streamlining the usage of DL in DIA research and applica- tions. The core LayoutParser library comes with a set of simple and intuitive interfaces for applying and customizing DL models for layout de- tection, character recognition, and many other document processing tasks. To promote extensibility, LayoutParser also incorporates a community platform for sharing both pre-trained models and full document digiti- zation pipelines. We demonstrate that LayoutParser is helpful for both lightweight and large-scale digitization pipelines in real-word use cases. The library is publicly available at https://layout-parser.github.io.\\n\\nKeywords: Document Image Analysis · Deep Learning · Layout Analysis · Character Recognition · Open Source library · Toolkit.')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdjS77Fh0oKJ",
        "outputId": "c5f60ea9-d756-40bd-a7ee-3724458ab9be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 2 0 2\n",
            "\n",
            "n u J 1 2 ] V C . s c [\n",
            "\n",
            "2 v 8 4 3 5 1 . 3 0 1 2 : v i X r a\n",
            "\n",
            "LayoutParser: A Uniﬁed Toolkit for Deep Learning Based Document Image Analysis\n",
            "\n",
            "Zejiang Shen! (4), Ruochen Zhang”, Melissa Dell?, Benjamin Charles Germain Lee*, Jacob Carlson’, and Weining Li®\n",
            "\n",
            "1 Allen Institute for AI shannons@allenai.org 2 Brown University ruochen zhang@brown.edu 3 Harvard University {melissadell,jacob carlson}@fas.harvard.edu 4 University of Washington bcgl@cs.washington.edu 5 University of Waterloo w422li@uwaterloo.ca\n",
            "\n",
            "Abstract. Recent advances in document image analysis (DIA) have been primarily driven by the application of neural networks. Ideally, research outcomes could be easily deployed in production and extended for further investigation. However, various factors like loosely organized codebases and sophisticated model conﬁgurations complicate the easy reuse of im- portant innovations by a wide audience. Though there have been on-going eﬀorts to improve reusability and simplify deep learning (DL) model development in disciplines like natural language processing and computer vision, none of them are optimized for challenges in the domain of DIA. This represents a major gap in the existing toolkit, as DIA is central to academic research across a wide range of disciplines in the social sciences and humanities. This paper introduces LayoutParser, an open-source library for streamlining the usage of DL in DIA research and applica- tions. The core LayoutParser library comes with a set of simple and intuitive interfaces for applying and customizing DL models for layout de- tection, character recognition, and many other document processing tasks. To promote extensibility, LayoutParser also incorporates a community platform for sharing both pre-trained models and full document digiti- zation pipelines. We demonstrate that LayoutParser is helpful for both lightweight and large-scale digitization pipelines in real-word use cases. The library is publicly available at https://layout-parser.github.io.\n",
            "\n",
            "Keywords: Document Image Analysis · Deep Learning · Layout Analysis · Character Recognition · Open Source library · Toolkit.\n"
          ]
        }
      ],
      "source": [
        "print(data[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "six4KwFGPX-S",
        "outputId": "549d7c87-0d86-474c-ea77-249d066f3bed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': './layoutparser_paper.pdf', 'last_modified': '2023-01-23T09:15:33', 'text_as_html': \"<table><thead><th>Dataset</th><th>| Base Model'|</th><th>| Notes</th></thead><tr><td>PubLayNet B8]|</td><td>F/M</td><td>Layouts of modern scientific documents</td></tr><tr><td>PRImA</td><td>M</td><td>Layouts of scanned modern magazines and scientific report</td></tr><tr><td>Newspaper</td><td>F</td><td>Layouts of scanned US newspapers from the 20th century</td></tr><tr><td>TableBank</td><td>F</td><td>Table region on modern scientific and business document</td></tr><tr><td>HJDataset</td><td>F/M</td><td>Layouts of history Japanese documents</td></tr></table>\", 'table_as_cells': [{'x': 0, 'y': 0, 'w': 1, 'h': 1, 'content': 'Dataset'}, {'x': 0, 'y': 1, 'w': 1, 'h': 1, 'content': 'PubLayNet B8]|'}, {'x': 0, 'y': 2, 'w': 1, 'h': 1, 'content': 'PRImA'}, {'x': 0, 'y': 3, 'w': 1, 'h': 1, 'content': 'Newspaper'}, {'x': 0, 'y': 4, 'w': 1, 'h': 1, 'content': 'TableBank'}, {'x': 0, 'y': 5, 'w': 1, 'h': 1, 'content': 'HJDataset'}, {'x': 1, 'y': 0, 'w': 1, 'h': 1, 'content': \"| Base Model'|\"}, {'x': 1, 'y': 1, 'w': 1, 'h': 1, 'content': 'F/M'}, {'x': 1, 'y': 2, 'w': 1, 'h': 1, 'content': 'M'}, {'x': 1, 'y': 3, 'w': 1, 'h': 1, 'content': 'F'}, {'x': 1, 'y': 4, 'w': 1, 'h': 1, 'content': 'F'}, {'x': 1, 'y': 5, 'w': 1, 'h': 1, 'content': 'F/M'}, {'x': 2, 'y': 0, 'w': 1, 'h': 1, 'content': '| Notes'}, {'x': 2, 'y': 1, 'w': 1, 'h': 1, 'content': 'Layouts of modern scientific documents'}, {'x': 2, 'y': 2, 'w': 1, 'h': 1, 'content': 'Layouts of scanned modern magazines and scientific report'}, {'x': 2, 'y': 3, 'w': 1, 'h': 1, 'content': 'Layouts of scanned US newspapers from the 20th century'}, {'x': 2, 'y': 4, 'w': 1, 'h': 1, 'content': 'Table region on modern scientific and business document'}, {'x': 2, 'y': 5, 'w': 1, 'h': 1, 'content': 'Layouts of history Japanese documents'}], 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'orig_elements': 'eJydVm1vmzAQ/isnvuxLk/CShKSqKq2aoq1qqmrtPmUIOXBJ0MAgbNRmbf/7zk5CaYCSTiQyPvzc+R4/d7B4NjDGBLn0o9A4B8Mds2A8HTk9dzl1e8PAtHrTYIi9YOK4rmuO3LEZGmdgJChZyCQjzLMRpGkeRpxJFHoes21aSH+D0XojyWLbpkmYvfkxCuWGrJarrVkacalwi4XjTvujM3AmTt/0zuAwH7lmf6zmlmNb/WGDYYcgiyG2QmKiMrmLnjC+z1iAxis9CFFiIKOU+0HMhPCzPF3SMrM/Ne2JRQtWUYxym6HG3s0NvWG+LthaZ7UwkK8NT1uF9JM0jFYRas5s03Z6RJTtPJjTc2t07jgKnRHS50WyxJxWUR6GZMsYfSb8AONYO1XccUn0Kz/fiE+BUmE1PzQ+7scntVUat2qkdN7h7orlDdveooSriffSBbdq8J8/kq9dKPsYdYuPImMZ5daBdI6RD4qFK8b/dCGHx8jv1ydSNDpGvsAV4WCehhh/aaXIamN4Nph3YWq0diJqlM66EDUqOxE1Ck/IpIG821SVdgvObmPtRte7gHQFVC6YcxBBRM+ocAII06BQfafTbY3YilsRMM4xPLhP2Jr9jTgKYDysBssxS/NW1dhtB9IQ6dc98IPyBazyNAG5QbBNuYGAYEW+7QrTXBC0xzV1J6BfnSyVzrIQKjVRMtcVp3b2lXQ2kZBpvoVrljFyiqcfx+hV9UCJT1I1so1MYuX6Qre2ywvigoVquNxX6sWA7tX8fQVWzFpbu/ngAM/pH16+72v0NNRmknB5f5rEdssH2u3etep5pZdGf5/TVj1E2SDfNv5RmBOEVY9RttKGGP+pqnqQsut2nUC7qCpOBzul6HeyUlHlxVdRiAU3LF/vJzuJwJscFs7EA32EdOtByTR9ELgelKTQlNaV26e1lgczGMCcrpm+dvc9umb0b1XT72K1NK2KouCTQjk42ImlEX6CAODjMz0EaTxXOLn6Dx9BOpjx6v0D180QhQ==', 'file_directory': '.', 'filename': 'layoutparser_paper.pdf', 'category': 'Table', 'element_id': '720a11c7a3fa16628248e6b9613d2c2d'}, page_content='Dataset Base Model1 Large Model Notes PubLayNet [38] PRImA [3] Newspaper [17] TableBank [18] HJDataset [31] F / M M F F F / M M - - F - Layouts of modern scientiﬁc documents Layouts of scanned modern magazines and scientiﬁc reports Layouts of scanned US newspapers from the 20th century Table region on modern scientiﬁc and business document Layouts of history Japanese documents')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "data[5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "AYe9e5K3Pfje",
        "outputId": "e63f8a62-ec9f-4fa9-c5af-3ebd0c240862"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Dataset Base Model1 Large Model Notes PubLayNet [38] PRImA [3] Newspaper [17] TableBank [18] HJDataset [31] F / M M F F F / M M - - F - Layouts of modern scientiﬁc documents Layouts of scanned modern magazines and scientiﬁc reports Layouts of scanned US newspapers from the 20th century Table region on modern scientiﬁc and business document Layouts of history Japanese documents'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "data[5].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "nFI5BBDjPoHK",
        "outputId": "b99cbdbb-717c-4661-f8e2-78466b7ed43d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table><thead><th>Dataset</th><th>| Base Model'|</th><th>| Notes</th></thead><tr><td>PubLayNet B8]|</td><td>F/M</td><td>Layouts of modern scientific documents</td></tr><tr><td>PRImA</td><td>M</td><td>Layouts of scanned modern magazines and scientific report</td></tr><tr><td>Newspaper</td><td>F</td><td>Layouts of scanned US newspapers from the 20th century</td></tr><tr><td>TableBank</td><td>F</td><td>Table region on modern scientific and business document</td></tr><tr><td>HJDataset</td><td>F/M</td><td>Layouts of history Japanese documents</td></tr></table>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "HTML(data[5].metadata['text_as_html'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbzDKmPzSA1H"
      },
      "source": [
        "Load using raw unstructured.io APIs for PDFs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_-wpOxHUMwOw"
      },
      "outputs": [],
      "source": [
        "from unstructured.partition.pdf import partition_pdf\n",
        "\n",
        "# Get elements - takes 3-4 mins\n",
        "raw_pdf_elements = partition_pdf(\n",
        "    filename=\"./layoutparser_paper.pdf\",\n",
        "    strategy='hi_res',\n",
        "    # Unstructured first finds embedded image blocks\n",
        "    extract_images_in_pdf=False,\n",
        "    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
        "    # Titles are any sub-section of the document\n",
        "    infer_table_structure=True,\n",
        "    # Post processing to aggregate text once we have the title\n",
        "    chunking_strategy=\"by_title\",\n",
        "    # Chunking params to aggregate text blocks\n",
        "    # Attempt to create a new chunk 3800 chars\n",
        "    # Attempt to keep chunks > 2000 chars\n",
        "    max_characters=4000,\n",
        "    new_after_n_chars=3800,\n",
        "    combine_text_under_n_chars=2000,\n",
        "    image_output_dir_path=\"./\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7QC4wt-St_K",
        "outputId": "e78472ce-6a25-4c43-eac7-4c7331437c92"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(raw_pdf_elements)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bC6seR7TSZY4",
        "outputId": "d47357c0-14ce-4a4a-b446-270df12da9cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<unstructured.documents.elements.CompositeElement at 0x7f95956b9890>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x7f959386f390>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x7f95823bf050>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x7f96bc86de90>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x7f96babb5410>,\n",
              " <unstructured.documents.elements.Table at 0x7f96bc893450>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x7f9595433e90>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x7f95954fbf90>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x7f96be6c8050>,\n",
              " <unstructured.documents.elements.Table at 0x7f9582420e10>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x7f95968bab90>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x7f95968b9150>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x7f95968b8bd0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x7f95968b9250>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x7f95968bb710>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x7f95968b88d0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x7f95968ba310>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x7f958242c690>]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "raw_pdf_elements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iPAwY5YS0CC",
        "outputId": "e47bc982-48d0-4f95-bcea-535da97246ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'type': 'Table',\n",
              " 'element_id': '720a11c7a3fa16628248e6b9613d2c2d',\n",
              " 'text': 'Dataset Base Model1 Large Model Notes PubLayNet [38] PRImA [3] Newspaper [17] TableBank [18] HJDataset [31] F / M M F F F / M M - - F - Layouts of modern scientiﬁc documents Layouts of scanned modern magazines and scientiﬁc reports Layouts of scanned US newspapers from the 20th century Table region on modern scientiﬁc and business document Layouts of history Japanese documents',\n",
              " 'metadata': {'last_modified': '2023-01-23T09:15:33',\n",
              "  'text_as_html': \"<table><thead><th>Dataset</th><th>| Base Model'|</th><th>| Notes</th></thead><tr><td>PubLayNet B8]|</td><td>F/M</td><td>Layouts of modern scientific documents</td></tr><tr><td>PRImA</td><td>M</td><td>Layouts of scanned modern magazines and scientific report</td></tr><tr><td>Newspaper</td><td>F</td><td>Layouts of scanned US newspapers from the 20th century</td></tr><tr><td>TableBank</td><td>F</td><td>Table region on modern scientific and business document</td></tr><tr><td>HJDataset</td><td>F/M</td><td>Layouts of history Japanese documents</td></tr></table>\",\n",
              "  'table_as_cells': [{'x': 0, 'y': 0, 'w': 1, 'h': 1, 'content': 'Dataset'},\n",
              "   {'x': 0, 'y': 1, 'w': 1, 'h': 1, 'content': 'PubLayNet B8]|'},\n",
              "   {'x': 0, 'y': 2, 'w': 1, 'h': 1, 'content': 'PRImA'},\n",
              "   {'x': 0, 'y': 3, 'w': 1, 'h': 1, 'content': 'Newspaper'},\n",
              "   {'x': 0, 'y': 4, 'w': 1, 'h': 1, 'content': 'TableBank'},\n",
              "   {'x': 0, 'y': 5, 'w': 1, 'h': 1, 'content': 'HJDataset'},\n",
              "   {'x': 1, 'y': 0, 'w': 1, 'h': 1, 'content': \"| Base Model'|\"},\n",
              "   {'x': 1, 'y': 1, 'w': 1, 'h': 1, 'content': 'F/M'},\n",
              "   {'x': 1, 'y': 2, 'w': 1, 'h': 1, 'content': 'M'},\n",
              "   {'x': 1, 'y': 3, 'w': 1, 'h': 1, 'content': 'F'},\n",
              "   {'x': 1, 'y': 4, 'w': 1, 'h': 1, 'content': 'F'},\n",
              "   {'x': 1, 'y': 5, 'w': 1, 'h': 1, 'content': 'F/M'},\n",
              "   {'x': 2, 'y': 0, 'w': 1, 'h': 1, 'content': '| Notes'},\n",
              "   {'x': 2,\n",
              "    'y': 1,\n",
              "    'w': 1,\n",
              "    'h': 1,\n",
              "    'content': 'Layouts of modern scientific documents'},\n",
              "   {'x': 2,\n",
              "    'y': 2,\n",
              "    'w': 1,\n",
              "    'h': 1,\n",
              "    'content': 'Layouts of scanned modern magazines and scientific report'},\n",
              "   {'x': 2,\n",
              "    'y': 3,\n",
              "    'w': 1,\n",
              "    'h': 1,\n",
              "    'content': 'Layouts of scanned US newspapers from the 20th century'},\n",
              "   {'x': 2,\n",
              "    'y': 4,\n",
              "    'w': 1,\n",
              "    'h': 1,\n",
              "    'content': 'Table region on modern scientific and business document'},\n",
              "   {'x': 2,\n",
              "    'y': 5,\n",
              "    'w': 1,\n",
              "    'h': 1,\n",
              "    'content': 'Layouts of history Japanese documents'}],\n",
              "  'filetype': 'application/pdf',\n",
              "  'languages': ['eng'],\n",
              "  'page_number': 5,\n",
              "  'orig_elements': 'eJydVm1vmzAQ/isnvuxLk/CSNKSqKq2aoq1qq2rtPmUIGbgkaGAQNmqztv99ZyehNEBJJxIZH37ufI+fO1g8G5hgilz6cWScgWFOJq5rOe6AjSNzMA7sYOCysTkIpxY6s7F1ao/ROAEjRckiJhlhno0wy4oo5kyi0POEbbJS+muMV2tJFts2TcLszI9xJNdktabammcxlwq3WDjT2XByAo7rDE3vBPbzydQcnqq55djWcNxi2CLIYoiNkJiqTO7iJ0zucxai8UoPIpQYyjjjfpgwIfy8yAJaZg5npu1atGAZJyg3OWrs3Y2hN8xXJVvprBYG8pXhaauQfppF8TJGzZlt2s7AtAa282DOzqzJmeModE5In5dpgAWtojwMyYIEfSb8EJNEO1XccUn0Kz/fiE+BUmE1PzQ+7sYntVUaN2qkdN7h7srgmm1uUcKl6730wa0G/OeP9Gsfyj5E3eKjyFlOufUgnUPkg2LhkvE/fcjxIfL71ZEUTQ6RL3BJOLjJIky+dFJkdTE8H930YRq09iIalM77EA0qexENCo/IpIW820yVdgfO7mLtWte7gGwJVC5YcBBhTM+ocEKIsrBUfafXbYPYmlsRMs4x2rtP2Yr9jTkKYDyqByswz4pO1dhdB9IS6dc98L3yBSyLLAW5RrBNuYaQYGWx6QvTXhC0xxV1J6BfkyyVTlAKlZqomOuL0zj7WjrrWMis2MAVyxk5xeOPY/KqeqDEJ6ka2VqmiXJ9rlvbxTlxwSI1XOwq9XxE92r+vgJrZq2t7Xy0hxf0jy7e9zV6GmkzSbi6P05i2+Uj7XbnWvW8ykurv89pqxmiapBvG/8ozBHCasaoWmlLjP9UVTNI1XX7TqBbVDWno61S9DtZqaj24qspxIJrVqx2k61E4E0OC8f1QB8h3XpQMU0fBFMPKlJoSuuq7dNay4M5jOCGrrm+tvcDuub071TT73IZmFZNUfBJoewdbMXSCj9CAPDxme6DtJ4rHF39+48gHcx49f4BnD4QZg==',\n",
              "  'file_directory': '.',\n",
              "  'filename': 'layoutparser_paper.pdf'}}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "raw_pdf_elements[5].to_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLKjlNBQTHwE"
      },
      "source": [
        "Convert into LangChain `document`format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCFjyenoTMC7",
        "outputId": "d7854ad6-c255-402f-f38c-7957af687d26"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'last_modified': '2023-01-23T09:15:33', 'text_as_html': \"<table><thead><th>Dataset</th><th>| Base Model'|</th><th>| Notes</th></thead><tr><td>PubLayNet B8]|</td><td>F/M</td><td>Layouts of modern scientific documents</td></tr><tr><td>PRImA</td><td>M</td><td>Layouts of scanned modern magazines and scientific report</td></tr><tr><td>Newspaper</td><td>F</td><td>Layouts of scanned US newspapers from the 20th century</td></tr><tr><td>TableBank</td><td>F</td><td>Table region on modern scientific and business document</td></tr><tr><td>HJDataset</td><td>F/M</td><td>Layouts of history Japanese documents</td></tr></table>\", 'table_as_cells': [{'x': 0, 'y': 0, 'w': 1, 'h': 1, 'content': 'Dataset'}, {'x': 0, 'y': 1, 'w': 1, 'h': 1, 'content': 'PubLayNet B8]|'}, {'x': 0, 'y': 2, 'w': 1, 'h': 1, 'content': 'PRImA'}, {'x': 0, 'y': 3, 'w': 1, 'h': 1, 'content': 'Newspaper'}, {'x': 0, 'y': 4, 'w': 1, 'h': 1, 'content': 'TableBank'}, {'x': 0, 'y': 5, 'w': 1, 'h': 1, 'content': 'HJDataset'}, {'x': 1, 'y': 0, 'w': 1, 'h': 1, 'content': \"| Base Model'|\"}, {'x': 1, 'y': 1, 'w': 1, 'h': 1, 'content': 'F/M'}, {'x': 1, 'y': 2, 'w': 1, 'h': 1, 'content': 'M'}, {'x': 1, 'y': 3, 'w': 1, 'h': 1, 'content': 'F'}, {'x': 1, 'y': 4, 'w': 1, 'h': 1, 'content': 'F'}, {'x': 1, 'y': 5, 'w': 1, 'h': 1, 'content': 'F/M'}, {'x': 2, 'y': 0, 'w': 1, 'h': 1, 'content': '| Notes'}, {'x': 2, 'y': 1, 'w': 1, 'h': 1, 'content': 'Layouts of modern scientific documents'}, {'x': 2, 'y': 2, 'w': 1, 'h': 1, 'content': 'Layouts of scanned modern magazines and scientific report'}, {'x': 2, 'y': 3, 'w': 1, 'h': 1, 'content': 'Layouts of scanned US newspapers from the 20th century'}, {'x': 2, 'y': 4, 'w': 1, 'h': 1, 'content': 'Table region on modern scientific and business document'}, {'x': 2, 'y': 5, 'w': 1, 'h': 1, 'content': 'Layouts of history Japanese documents'}], 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'orig_elements': 'eJydVm1vmzAQ/isnvuxLk/CS16qqtGqKtqqpqrX7lCFksANoYBA2arO2/31nJ6E0QEknEhkffu58j587WD8bLGEp49KLqXEOxng6G9PpYjKwTUIH49ncGRB/MR3QsW8uCJ0ElrUwzsBImSSUSIKYZyPIsoLGnEgm9Dwh26yUXsTiMJJosW3TRMze/BhTGaHVmmlrnsVcKtx67cwWw8kZOHNnaLpncJhPZuZwquaWY1vDcYthh0CLIbZCslRlchc/seQ+JwEzXvEBZZIFMs64FyRECC8vMh+XmcOFac8tXLCJEya3OdPYu5WhN8zDkoQ6q7XBeGi42iqkl2Y03sRMc2abtjMwrYHtPJiLc2ty7jgKnSPS42XqswJXYR6GJH7CPCK8gCWJdqq44xLpV36+IZ+CSYXV/OD4uB+f1FZx3KoR03mHuyv9G7K9ZRKu5u5LH9xqwH/+SL/2oexj1C17FDnJMbcepHOMfFAsXBH+pw85PkZ+vz6Roskx8gWuEAerjLLkSydFVhfDy9GqD9OgtRfRoHTZh2hQ2YtoUHhCJi3k3WaqtDtwdhdrN7reBWQbwHJhBQcRxPgMCycAmgWl6ju9bhvE1tyKgHDO6MF9SkLyN+ZMAOG0HqxgeVZ0qsbuOpCWSL/ugR+UL2BTZCnIiIFtyggChJXFti9Me0HgHkPsToC/JlkqHb8UKjVRMdcXp3H2tXSiWMis2MI1yQk6Zacfx+RV9UDJnqRqZJFME+X6Qre2ywvkglA1XO4r9WKE92r+vgJrZq2t3Xx0gBf4p5fv+xo+pdqMEq7uT5PYbvlIu927Vj2v8tLq73PaaoaoGuTbxj8Kc4KwmjGqVtoS4z9V1QxSdd2+E+gWVc3paKcU/U5WKqq9+GoKseCGFOF+spMIvMlh7cxd0EeIty5UTOMHwcyFihSc4rpq+7jWcmEJI1jhtdTX7n6A1xL/nWr6XW5806opCj4plIODnVha4ScIAD4+00OQ1nOFk6v/8BGkgxmv7j/xDRDV', 'file_directory': '.', 'filename': 'layoutparser_paper.pdf'}, page_content='Dataset Base Model1 Large Model Notes PubLayNet [38] PRImA [3] Newspaper [17] TableBank [18] HJDataset [31] F / M M F F F / M M - - F - Layouts of modern scientiﬁc documents Layouts of scanned modern magazines and scientiﬁc reports Layouts of scanned US newspapers from the 20th century Table region on modern scientiﬁc and business document Layouts of history Japanese documents')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "lc_docs = [Document(page_content=doc.text,\n",
        "                    metadata=doc.metadata.to_dict())\n",
        "              for doc in raw_pdf_elements]\n",
        "lc_docs[5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQV0X4grW971"
      },
      "source": [
        "### Microsoft Office Document Loaders\n",
        "\n",
        "The Microsoft Office suite of productivity software includes Microsoft Word, Microsoft Excel, Microsoft PowerPoint, Microsoft Outlook, and Microsoft OneNote. It is available for Microsoft Windows and macOS operating systems. It is also available on Android and iOS.\n",
        "\n",
        "[Unstructured.io](https://docs.unstructured.io/open-source/introduction/overview) provides a variety of document loaders to load MS Office documents. Check them out [here](https://docs.unstructured.io/open-source/core-functionality/partitioning).\n",
        "\n",
        "Here we will leverage LangChain's [`UnstructuredWordDocumentLoader`](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.word_document.UnstructuredWordDocumentLoader.html) to load data from a MS Word document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V37elBlCT6Mn",
        "outputId": "a24c9831-e679-4fd0-f844-736bfb2ae610"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1DEz13a7k4yX9yFrWaz3QJqHdfecFYRV-\n",
            "To: /content/Quantum Computing.docx\n",
            "\r  0% 0.00/11.4k [00:00<?, ?B/s]\r100% 11.4k/11.4k [00:00<00:00, 21.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1DEz13a7k4yX9yFrWaz3QJqHdfecFYRV-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv2b1SZcX4zS"
      },
      "source": [
        "Load word doc as a single document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "BqYBSoqHT6TY"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import UnstructuredWordDocumentLoader\n",
        "\n",
        "loader = UnstructuredWordDocumentLoader('./Quantum Computing.docx')\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfi8_V9MT6Vl",
        "outputId": "92036144-cc49-4f7c-801f-8406cbb46939"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "kznFjlj6T6Xw",
        "outputId": "4473df91-0106-47d1-d7b1-632ceaf666ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Rise of Quantum Computing: A New Era of Innovation\\n\\nFor decades, classical computing has driven technological advancements, but the limitations of traditional binary processing are becoming evident as the world demands more computational power. Enter quantum computing—a revolutionary approach that leverages the principles of quantum mechanics to solve complex problems at unprecedented speeds.\\n\\nUnderstanding Quantum Computing\\n\\nUnlike classical computers that process information using bits (0s and 1s), quantum computers use qubits, which can exist in multiple states simultaneously due to superposition. This unique property allows quantum systems to process vast amounts of data in parallel, making them exponentially more powerful for specific tasks.\\n\\nAnother key principle, entanglement, enables qubits to be interconnected, meaning the state of one qubit is dependent on another, regardless of distance. This drastically enhances processing efficiency and speed, paving the way for breakt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "data[0].page_content[:1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILz-pXIyX8e0"
      },
      "source": [
        "Load word doc with complex parsing and section based chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "7cKYWNjAUo1A"
      },
      "outputs": [],
      "source": [
        "loader = UnstructuredWordDocumentLoader('./Quantum Computing.docx',\n",
        "                                        strategy='fast',\n",
        "                                        chunking_strategy=\"by_title\",\n",
        "                                        max_characters=3000, # max limit of a document chunk\n",
        "                                        new_after_n_chars=2500, # preferred document chunk size\n",
        "                                        mode='elements')\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkOn6t0QVOyE",
        "outputId": "f28b0315-4339-4582-9466-6ec756acb2bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IV4cp6yqVQZ1",
        "outputId": "cfa26ae4-c908-45f1-be81-5134562d322f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': './Quantum Computing.docx', 'emphasized_text_contents': ['Understanding Quantum Computing', 'qubits', 'superposition', 'entanglement', 'Applications Transforming Industries', 'Drug Discovery & Healthcare', 'Financial Modeling', 'Cybersecurity & Cryptography', 'post-quantum cryptography', 'Climate Modeling & Sustainability', 'AI & Machine Learning'], 'emphasized_text_tags': ['b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b'], 'file_directory': '.', 'filename': 'Quantum Computing.docx', 'languages': ['eng'], 'last_modified': '2025-02-19T14:45:48', 'orig_elements': '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', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category': 'CompositeElement', 'element_id': 'ebbfc67ada357945d491043bfb0293aa'}, page_content='The Rise of Quantum Computing: A New Era of Innovation\\n\\nFor decades, classical computing has driven technological advancements, but the limitations of traditional binary processing are becoming evident as the world demands more computational power. Enter quantum computing—a revolutionary approach that leverages the principles of quantum mechanics to solve complex problems at unprecedented speeds.\\n\\nUnderstanding Quantum Computing\\n\\nUnlike classical computers that process information using bits (0s and 1s), quantum computers use qubits, which can exist in multiple states simultaneously due to superposition. This unique property allows quantum systems to process vast amounts of data in parallel, making them exponentially more powerful for specific tasks.\\n\\nAnother key principle, entanglement, enables qubits to be interconnected, meaning the state of one qubit is dependent on another, regardless of distance. This drastically enhances processing efficiency and speed, paving the way for breakthroughs in cryptography, materials science, and artificial intelligence.\\n\\nApplications Transforming Industries\\n\\nDrug Discovery & Healthcare\\nQuantum simulations can analyze molecular structures and interactions at an atomic level, accelerating drug discovery and personalized medicine. Companies like IBM and Google are already exploring quantum approaches to fight diseases like cancer and Alzheimer’s.\\n\\nFinancial Modeling\\nFinancial markets involve complex, unpredictable variables. Quantum algorithms can optimize portfolios, manage risk, and predict market fluctuations with greater accuracy than classical computing methods.\\n\\nCybersecurity & Cryptography\\nTraditional encryption methods rely on mathematical complexity, which quantum computers could break in seconds. This has sparked the rise of post-quantum cryptography, aimed at developing secure algorithms resistant to quantum attacks.\\n\\nClimate Modeling & Sustainability\\nQuantum computing can process massive climate datasets, helping scientists model climate change scenarios with greater precision. This will improve renewable energy optimization, disaster prediction, and environmental impact assessment.\\n\\nAI & Machine Learning\\nQuantum-enhanced AI models can process data faster, recognize patterns more efficiently, and revolutionize deep learning architectures, leading to advancements in robotics, automation, and natural language processing.')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mW5UhN9BVTaE",
        "outputId": "c272bca1-82a6-4572-ea72-741470ea842d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': './Quantum Computing.docx', 'emphasized_text_contents': ['Challenges & The Road Ahead', 'Quantum Revolution'], 'emphasized_text_tags': ['b', 'b'], 'file_directory': '.', 'filename': 'Quantum Computing.docx', 'languages': ['eng'], 'last_modified': '2025-02-19T14:45:48', 'orig_elements': 'eJzVVE1v3DYQ/SsDHXpaGdJa8q58C1KgOQVo4FsaLIbkSCJCkQo/1tkG/e+dke0kSAK3vTUXQeR8v/c4bz9V5Gghn0/WVLdQ7YdWazJUj+pg6u6mbWtFStfjtel7vR/401Y7qBbKaDAjx3yqNGaaQrycDK155quWPWhZZ0z2TzKnTB/zSQefuU5i89vq5YzOkZ8owS9wNxO8CWjgxUxoqnc/CM44PQSqzTxaRydjI+nMZaXvq+rx2uNCcvF7QZ/LAi/DspZs/XRlgv4oXg79VJBLbwm5iS2lw5RPSzB2tPSARLPv62Zft8Nd2912/W13rP5iR+lH7M+NIG6XdevjzmZHEvgt0qi00Qqx3uORkR411YPiv/1xVF3fHa7x5vDPSDf/EzR+pbTaTGBzgjWGxSbawYfHqvqpKoyoGa9kJ8+ZNVtBf4ZxB9ZrV4z4fSjKZkgZlXU2X3ZAMYbIiaJMaYPfAXoDDuNEddLoCGaM5h4jgaEzubAK1FfwKtzzMe7g3uYZRIPWl1ASqEj4Ps8xlGlOXBlSWSmygyk6f26Bm8oR15UMcNH0UDWHNbgwcf/u+xG5EoG0sYSzpNEuJIocAyhOC9ewPPEFzhYVt/2UYCy5RLr6WjqvMUbM9kx3AvIPJKSH3qDWTU1D39TdsVX1gHxsu2G81t3BYNP9PBL6LTBRXsZLDzCTnmGyuJ0ZUOvPlDZqWBRO6OArxvUJwUiJMOqZxeJTieKYZ8z8sUlY9GkMcdkA3XJ7IfHCwnBOYmdcpYYpKUcrakzaci+iUzA2aenu8igAfu3GTjazAHh4wwLcYuEeL0n4n5FrXCgL7XbByfr/yCzioSPSfT0ON6ru+uZQH695Kw8HxLbThI3a/ztmn13DT5y+oXNwRd7Vz7J9X2w484JgOoLfGFFM+XsI43b4frId+5GIgXXBgtCOxXK7+XrOCWOU3cAvlRN82Vfs6AO4wPsp8gNmHWRZWoaXx/JH2Tdtx2uKnRRxjASwQRbWc2y/+xsLIoCY', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category': 'CompositeElement', 'element_id': 'cad0182fba34f6c6d546a11eb4444c39'}, page_content='Challenges & The Road Ahead\\n\\nDespite its promise, quantum computing faces significant challenges, including qubit stability, error correction, and large-scale hardware development. However, with continuous breakthroughs in superconducting qubits, trapped ions, and topological quantum computing, we are moving closer to a commercially viable quantum future.\\n\\nGovernments and tech giants are investing billions into quantum research, ensuring that this transformative technology will reshape industries, scientific discovery, and the digital landscape in ways we have yet to imagine.\\n\\nAs we stand on the brink of the Quantum Revolution, one thing is clear: the next frontier of computing is no longer a distant dream—it is becoming reality.')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "data[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewBJD6nbfYxg"
      },
      "source": [
        "### Directory Loaders\n",
        "\n",
        "LangChain's [`DirectoryLoader`](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.directory.DirectoryLoader.html) implements functionality for reading files from disk into LangChain [`Document`](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document) objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uvblyc6OZqpe",
        "outputId": "9eccde9d-171a-4e6c-9af9-9ca86223c899"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-29 12:11:49--  https://arxiv.org/pdf/2010.11929.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.195.42, 151.101.67.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2010.11929 [following]\n",
            "--2025-05-29 12:11:49--  http://arxiv.org/pdf/2010.11929\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3743814 (3.6M) [application/pdf]\n",
            "Saving to: ‘Vision Transformers.pdf’\n",
            "\n",
            "\rVision Transformers   0%[                    ]       0  --.-KB/s               \rVision Transformers 100%[===================>]   3.57M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2025-05-29 12:11:49 (43.5 MB/s) - ‘Vision Transformers.pdf’ saved [3743814/3743814]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O 'Vision Transformers.pdf' 'https://arxiv.org/pdf/2010.11929.pdf'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCXVqm3Ba2I8"
      },
      "source": [
        "We first define and assign specific loaders which can be used by LangChain when processing the files for a specific file type. We follow this format\n",
        "\n",
        "```\n",
        "loaders = {\n",
        "  'file_format_extension' : (LoaderClass, LoaderKeywordArguments)\n",
        "}\n",
        "```\n",
        "\n",
        "Where:\n",
        "\n",
        "- `file_format_extension` can be anything like `.docx`, `.pdf`etc.\n",
        "- `LoaderClass` is a specific data loader like `PyMuPDFLoader`\n",
        "- `LoaderKeywordArguments` are any specific keyword arguments which needs to be passed into that loader at runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "1zzlBioka1HR"
      },
      "outputs": [],
      "source": [
        "# Define a dictionary to map file extensions to their respective loaders\n",
        "loaders = {\n",
        "    '.pdf': (PyMuPDFLoader, {}),\n",
        "    '.docx': (UnstructuredWordDocumentLoader, {'strategy': 'fast',\n",
        "                                              'chunking_strategy' : 'by_title',\n",
        "                                              'max_characters' : 3000, # max limit of a document chunk\n",
        "                                              'new_after_n_chars' : 2500, # preferred document chunk size\n",
        "                                              'mode' : 'elements'\n",
        "                                              })\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBJZJtsGbnr7"
      },
      "source": [
        "`DirectoryLoader` accepts a `loader_cls` argument, which defaults to `UnstructuredLoader` but we can pass our own loaders which we defined above in the `loader_cls`argument and any keyword args for the loader can be passed in the `loader_kwargs` argument.\n",
        "\n",
        "We can also show a progress bar by setting `show_progress=True`\n",
        "\n",
        "We can use the `glob` parameter to control which files to load based on file patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mOmj9K-cB14"
      },
      "source": [
        "Here we create two separate loaders to load files which are word documents and PDFs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbnt25HdXmkc",
        "outputId": "106af0c4-c246-42ed-bc2c-4428bfb013d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00,  7.99it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 17.15it/s]\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "\n",
        "# Define a function to create a DirectoryLoader for a specific file type\n",
        "def create_directory_loader(file_type, directory_path):\n",
        "    return DirectoryLoader(\n",
        "        path=directory_path,\n",
        "        glob=f\"**/*{file_type}\",\n",
        "        loader_cls=loaders[file_type][0],\n",
        "        loader_kwargs=loaders[file_type][1],\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "# Create DirectoryLoader instances for each file type\n",
        "pdf_loader = create_directory_loader('.pdf', './')\n",
        "docx_loader = create_directory_loader('.docx', './')\n",
        "\n",
        "# Load the files\n",
        "pdf_documents = pdf_loader.load()\n",
        "docx_documents = docx_loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXYc4p_eXmmt",
        "outputId": "90a2e804-bc32-4957-9d55-03fdbe3482e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "len(pdf_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ze6DA3dGaTln",
        "outputId": "5709c3e6-b2c2-4c1d-c4c6-55fd7634cc02"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'Vision Transformers.pdf', 'file_path': 'Vision Transformers.pdf', 'page': 18, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nwe perform timing of inference speed for the main models of interest, on a TPUv3 accelerator; the\\ndifference between inference and backprop speed is a constant model-independent factor.\\nFigure 12 (left) shows how many images one core can handle per second, across various input sizes.\\nEvery single point refers to the peak performance measured across a wide range of batch-sizes. As\\ncan be seen, the theoretical bi-quadratic scaling of ViT with image size only barely starts happening\\nfor the largest models at the largest resolutions.\\nAnother quantity of interest is the largest batch-size each model can ﬁt onto a core, larger being\\nbetter for scaling to large datasets. Figure 12 (right) shows this quantity for the same set of models.\\nThis shows that large ViT models have a clear advantage in terms of memory-efﬁciency over ResNet\\nmodels.\\n64\\n128\\n224\\n384\\n512\\nInput size [px]\\n102\\n103\\n104\\nPeak inference speed [img/sec/core]\\n64\\n128\\n224\\n384\\n512\\nInput size [px]\\n102\\n103\\nLargest per-core batch-size\\nR50x1\\nR50x2\\nViT-B/32\\nViT-L/32\\nViT-B/16\\nViT-L/16\\nViT-H/14\\nR152x4\\nFigure 12: Left: Real wall-clock timings of various architectures across input sizes. ViT models\\nhave speed comparable to similar ResNets. Right: Largest per-core batch-size ﬁtting on device with\\nvarious architectures across input sizes. ViT models are clearly more memory-efﬁcient.\\nD.6\\nAXIAL ATTENTION\\nAxial Attention (Huang et al., 2020; Ho et al., 2019) is a simple, yet effective technique to run self-\\nattention on large inputs that are organized as multidimensional tensors. The general idea of axial\\nattention is to perform multiple attention operations, each along a single axis of the input tensor,\\ninstead of applying 1-dimensional attention to the ﬂattened version of the input. In axial attention,\\neach attention mixes information along a particular axis, while keeping information along the other\\naxes independent. Along this line, Wang et al. (2020b) proposed the AxialResNet model in which\\nall the convolutions with kernel size 3 × 3 in a ResNet50 are replaced by axial self-attention, i.e.\\na row and column attention, augmented by relative positional encoding. We have implemented\\nAxialResNet as a baseline model.3.\\nMoreover, we have modiﬁed ViT to process inputs in the 2-dimensional shape, instead of a 1-\\ndimensional sequence of patches, and incorporate Axial Transformer blocks, in which instead of\\na self-attention followed by an MLP, we have a a row-self-attention plus an MLP followed by a\\ncolumn-self-attention plus an MLP.\\nFigure 13, present the performance of Axial ResNet, Axial-ViT-B/32 and Axial-ViT-B/16 on Ima-\\ngeNet 5shot linear, when pretrained on JFT dataset, verses the pretraining compute, both in terms of\\nnumber of FLOPs and inference time (example per seconds). As we can see, both Axial-ViT-B/32\\nand Axial-ViT-B/16 do better than their ViT-B counterpart in terms of performance, but it comes at\\n3Our implementation is based on the open-sourced PyTorch implementation in https://github.com/\\ncsrhddlam/axial-deeplab. In our experiments, we reproduced the scores reported in (Wang et al.,\\n2020b) in terms of accuracy, however, our implementation, similar to the open-source implementation, is very\\nslow on TPUs. Therefore, we were not able to use it for extensive large-scale experiments. These may be\\nunlocked by a carefully optimized implementation.\\n19\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "pdf_documents[18]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRVEmiTvXmpn",
        "outputId": "467582ec-578e-4297-8a23-eced87646fa0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "len(docx_documents)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a0bdf462f1644153b5c798268af9311c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3371035b0a8d4fe28bf49d98aa2f3a88",
              "IPY_MODEL_117e63248e10471d839261faa99eea04",
              "IPY_MODEL_a19a4a39337d48fbb74e46331f182a77"
            ],
            "layout": "IPY_MODEL_5eba90e93f0e47229e2379e47b032358"
          }
        },
        "3371035b0a8d4fe28bf49d98aa2f3a88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15c4feebb07b4b45b94dc9c40a4c7b2d",
            "placeholder": "​",
            "style": "IPY_MODEL_0c8f2c7239074fe4b5d44ad1f761e796",
            "value": "yolox_l0.05.onnx: 100%"
          }
        },
        "117e63248e10471d839261faa99eea04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1eb45ece889347bf95476f04fdad392c",
            "max": 216625723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_945d5eff4ffd4290ad28d806961a3797",
            "value": 216625723
          }
        },
        "a19a4a39337d48fbb74e46331f182a77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48f14f4cc68d405e9dc7c307219a8bd7",
            "placeholder": "​",
            "style": "IPY_MODEL_a9c844c92a694cc1bbbf999e26ee21d4",
            "value": " 217M/217M [00:01&lt;00:00, 162MB/s]"
          }
        },
        "5eba90e93f0e47229e2379e47b032358": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15c4feebb07b4b45b94dc9c40a4c7b2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c8f2c7239074fe4b5d44ad1f761e796": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1eb45ece889347bf95476f04fdad392c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "945d5eff4ffd4290ad28d806961a3797": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "48f14f4cc68d405e9dc7c307219a8bd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9c844c92a694cc1bbbf999e26ee21d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4e80d70186a4545ad05f3fba1bd193b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2ae4a96cbdf4170885482b8b6567d56",
              "IPY_MODEL_283786ae92814994b20f693c985fc0cd",
              "IPY_MODEL_bfd6723afa4246c2b75352a9ceb62dfc"
            ],
            "layout": "IPY_MODEL_3518f3f1394349b89aeefeeb64a240f9"
          }
        },
        "f2ae4a96cbdf4170885482b8b6567d56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_552216d666254e8c9a713880b75aa526",
            "placeholder": "​",
            "style": "IPY_MODEL_78bfb1f3370f4720a7917f437c4841c5",
            "value": "config.json: 100%"
          }
        },
        "283786ae92814994b20f693c985fc0cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6af5bd232f04b1f9d2d36f431dce328",
            "max": 1469,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f42767541924485abd58f6157c294af7",
            "value": 1469
          }
        },
        "bfd6723afa4246c2b75352a9ceb62dfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39e417f90aac486e8f77eeb7c985ab46",
            "placeholder": "​",
            "style": "IPY_MODEL_24eed9a6c7d54b39b2d514df57195f8a",
            "value": " 1.47k/1.47k [00:00&lt;00:00, 86.9kB/s]"
          }
        },
        "3518f3f1394349b89aeefeeb64a240f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "552216d666254e8c9a713880b75aa526": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78bfb1f3370f4720a7917f437c4841c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6af5bd232f04b1f9d2d36f431dce328": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f42767541924485abd58f6157c294af7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39e417f90aac486e8f77eeb7c985ab46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24eed9a6c7d54b39b2d514df57195f8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41256336abd74ce489b211456af4e90d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_96fede9942dc450ba1fdaabc6b912257",
              "IPY_MODEL_55138bba571b48288ae03f29b1b5112e",
              "IPY_MODEL_dd7c6a1a8c4e4c3a93e9e49c28734006"
            ],
            "layout": "IPY_MODEL_a79aba0edd8d40b5a93ade2ebb813b56"
          }
        },
        "96fede9942dc450ba1fdaabc6b912257": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0c9bd2809a04493ac4f2f9c9e33a2ff",
            "placeholder": "​",
            "style": "IPY_MODEL_cddc6f1bec414e68ba1ca17f0851a14c",
            "value": "model.safetensors: 100%"
          }
        },
        "55138bba571b48288ae03f29b1b5112e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6dd76543dd4942639111000843ee1a38",
            "max": 115434268,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d1f6d4bd712c4b0e811ab00883b114e3",
            "value": 115434268
          }
        },
        "dd7c6a1a8c4e4c3a93e9e49c28734006": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1ce222bef1546738b361789116e8599",
            "placeholder": "​",
            "style": "IPY_MODEL_378f1acbae6b4c5793e6d347b62f0330",
            "value": " 115M/115M [00:01&lt;00:00, 118MB/s]"
          }
        },
        "a79aba0edd8d40b5a93ade2ebb813b56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0c9bd2809a04493ac4f2f9c9e33a2ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cddc6f1bec414e68ba1ca17f0851a14c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6dd76543dd4942639111000843ee1a38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1f6d4bd712c4b0e811ab00883b114e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e1ce222bef1546738b361789116e8599": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "378f1acbae6b4c5793e6d347b62f0330": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1f023ff52a74f868c9f43a771327eca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b67ed462a32a45fabf5ff3c1382c164e",
              "IPY_MODEL_d6016b9615f24fde940a792da0fa23be",
              "IPY_MODEL_c4bdaebfdf9146d7a8990d80b60d5ccb"
            ],
            "layout": "IPY_MODEL_ae089cdcb18b4be5842eaf90660dcaaa"
          }
        },
        "b67ed462a32a45fabf5ff3c1382c164e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dccaef03f0294f258eb1111a01143a68",
            "placeholder": "​",
            "style": "IPY_MODEL_af3ea2fdf0234b67ba28d98519a6eb7c",
            "value": "model.safetensors: 100%"
          }
        },
        "d6016b9615f24fde940a792da0fa23be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5da13d72dbdf473a982b9c4c4649cf0d",
            "max": 46807446,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_51453d81251c4209bdcac4ae00564c0f",
            "value": 46807446
          }
        },
        "c4bdaebfdf9146d7a8990d80b60d5ccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b05479cc2ec430485e97961878686d5",
            "placeholder": "​",
            "style": "IPY_MODEL_b69894e2c5834c6ea3bb0f77162dc06c",
            "value": " 46.8M/46.8M [00:00&lt;00:00, 190MB/s]"
          }
        },
        "ae089cdcb18b4be5842eaf90660dcaaa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dccaef03f0294f258eb1111a01143a68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af3ea2fdf0234b67ba28d98519a6eb7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5da13d72dbdf473a982b9c4c4649cf0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51453d81251c4209bdcac4ae00564c0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2b05479cc2ec430485e97961878686d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b69894e2c5834c6ea3bb0f77162dc06c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}