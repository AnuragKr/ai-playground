{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "zTpl4potSmv2",
      "metadata": {
        "id": "zTpl4potSmv2"
      },
      "source": [
        "# Debugging, Evaluating and Monitoring LLM Applications with LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L1KvMtf54l0d",
      "metadata": {
        "id": "L1KvMtf54l0d"
      },
      "source": [
        "## Install OpenAI, and LangChain dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "052be842-e103-4c46-9247-7fd466643f4f",
      "metadata": {
        "id": "052be842-e103-4c46-9247-7fd466643f4f"
      },
      "source": [
        "Install the following httpx library version for compatibility with other libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b8bb528b-d0fc-4b01-a64c-855373de1e02",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8bb528b-d0fc-4b01-a64c-855373de1e02",
        "outputId": "af145baa-7adf-4d99-e768-c405b940c5eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting httpx==0.27.2\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx==0.27.2) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.27.2) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx==0.27.2) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx==0.27.2) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.27.2) (1.3.1)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx==0.27.2) (0.16.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx==0.27.2) (4.13.2)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: httpx\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.16.1 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed httpx-0.27.2\n"
          ]
        }
      ],
      "source": [
        "!pip install httpx==0.27.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2evPp14fy258",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2evPp14fy258",
        "outputId": "05827a15-b179-4a0e-a578-b21e0433a806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.2.0\n",
            "  Downloading langchain-0.2.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.0) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.0) (2.0.41)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.0) (3.11.15)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.2.0)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting langchain-core<0.3.0,>=0.2.0 (from langchain==0.2.0)\n",
            "  Downloading langchain_core-0.2.43-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain==0.2.0)\n",
            "  Downloading langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.2.0)\n",
            "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting numpy<2,>=1 (from langchain==0.2.0)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.0) (2.11.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.0) (2.32.3)\n",
            "Collecting tenacity<9.0.0,>=8.1.0 (from langchain==0.2.0)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0) (1.20.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.2.0)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.2.0)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain==0.2.0) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain==0.2.0) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain==0.2.0) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.0) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.0) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.0) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.2.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.2.0) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.2.0) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.2.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.2.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.2.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.2.0) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.2.0) (3.2.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.0) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.0) (1.0.9)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.0) (1.3.1)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.0) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain==0.2.0) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.2.0)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain-0.2.0-py3-none-any.whl (973 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.7/973.7 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_core-0.2.43-py3-none-any.whl (397 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.1/397.1 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.2.4-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: tenacity, numpy, mypy-extensions, marshmallow, typing-inspect, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.1.2\n",
            "    Uninstalling tenacity-9.1.2:\n",
            "      Successfully uninstalled tenacity-9.1.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.3.42\n",
            "    Uninstalling langsmith-0.3.42:\n",
            "      Successfully uninstalled langsmith-0.3.42\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.60\n",
            "    Uninstalling langchain-core-0.3.60:\n",
            "      Successfully uninstalled langchain-core-0.3.60\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.8\n",
            "    Uninstalling langchain-text-splitters-0.3.8:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.8\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.25\n",
            "    Uninstalling langchain-0.3.25:\n",
            "      Successfully uninstalled langchain-0.3.25\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-0.2.0 langchain-core-0.2.43 langchain-text-splitters-0.2.4 langsmith-0.1.147 marshmallow-3.26.1 mypy-extensions-1.1.0 numpy-1.26.4 tenacity-8.5.0 typing-inspect-0.9.0\n",
            "Collecting langchain-openai==0.1.7\n",
            "  Downloading langchain_openai-0.1.7-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.1.46 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.1.7) (0.2.43)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.1.7) (1.81.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.1.7) (0.9.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3,>=0.1.46->langchain-openai==0.1.7) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3,>=0.1.46->langchain-openai==0.1.7) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3,>=0.1.46->langchain-openai==0.1.7) (0.1.147)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3,>=0.1.46->langchain-openai==0.1.7) (24.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3,>=0.1.46->langchain-openai==0.1.7) (2.11.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3,>=0.1.46->langchain-openai==0.1.7) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3,>=0.1.46->langchain-openai==0.1.7) (4.13.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.1.7) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.1.7) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.46->langchain-openai==0.1.7) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.1.46->langchain-openai==0.1.7) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.1.46->langchain-openai==0.1.7) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.46->langchain-openai==0.1.7) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.46->langchain-openai==0.1.7) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.46->langchain-openai==0.1.7) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.1.7) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.1.7) (2.4.0)\n",
            "Downloading langchain_openai-0.1.7-py3-none-any.whl (34 kB)\n",
            "Installing collected packages: langchain-openai\n",
            "Successfully installed langchain-openai-0.1.7\n",
            "Collecting langchain-community==0.2.0\n",
            "  Downloading langchain_community-0.2.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.0) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.0) (2.0.41)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.0) (3.11.15)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.0) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.0) (0.2.0)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.0) (0.2.43)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.0) (0.1.147)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.0) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.0) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.0) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.0) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.0) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.0) (1.20.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.0) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.0) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain<0.3.0,>=0.2.0->langchain-community==0.2.0) (0.2.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain<0.3.0,>=0.2.0->langchain-community==0.2.0) (2.11.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community==0.2.0) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community==0.2.0) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community==0.2.0) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community==0.2.0) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community==0.2.0) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community==0.2.0) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.2.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.2.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.2.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.2.0) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.2.0) (3.2.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.0) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.0) (1.0.9)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.0) (1.3.1)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.0) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain-community==0.2.0) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community==0.2.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community==0.2.0) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community==0.2.0) (0.4.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.0) (1.1.0)\n",
            "Downloading langchain_community-0.2.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-community\n",
            "Successfully installed langchain-community-0.2.0\n",
            "Collecting langsmith==0.1.71\n",
            "  Downloading langsmith-0.1.71-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith==0.1.71) (3.10.18)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langsmith==0.1.71) (2.11.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith==0.1.71) (2.32.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith==0.1.71) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith==0.1.71) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith==0.1.71) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith==0.1.71) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith==0.1.71) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith==0.1.71) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith==0.1.71) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith==0.1.71) (2025.4.26)\n",
            "Downloading langsmith-0.1.71-py3-none-any.whl (124 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langsmith\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.1.147\n",
            "    Uninstalling langsmith-0.1.147:\n",
            "      Successfully uninstalled langsmith-0.1.147\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-core 0.2.43 requires langsmith<0.2.0,>=0.1.112, but you have langsmith 0.1.71 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langsmith-0.1.71\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain==0.2.0\n",
        "!pip install langchain-openai==0.1.7\n",
        "!pip install langchain-community==0.2.0\n",
        "!pip install langsmith==0.1.71"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1T0s0um5Svfa",
      "metadata": {
        "id": "1T0s0um5Svfa"
      },
      "source": [
        "## Setup Environment Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "x1YSuHNF_lbh",
      "metadata": {
        "id": "x1YSuHNF_lbh"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "# you can change this based on specific apps and projects\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"LLM App Project - 1\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGSMITH_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nNpAYqpLYEK3",
      "metadata": {
        "id": "nNpAYqpLYEK3"
      },
      "source": [
        "# Debugging and Monitoring LLM Apps with LangSmith\n",
        "\n",
        "We will look at various ways in which we can evaluate LLM App steps, also known as runs. Collection of runs make up a trace.\n",
        "\n",
        "A Trace is essentially a series of steps that your application takes to go from input to output.\n",
        "\n",
        "Each of these individual steps is represented by a Run.\n",
        "\n",
        "A Project is simply a collection of traces.\n",
        "\n",
        "![](https://i.imgur.com/hiMkTK9.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EQIYNKCxfF6_",
      "metadata": {
        "id": "EQIYNKCxfF6_"
      },
      "source": [
        "## Monitor LLM App Traces with LangSmith"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "FCTCIfDTy5bX",
      "metadata": {
        "id": "FCTCIfDTy5bX"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "# Configure the chat prompt template and define the llm chain.\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Act as a helpful AI Assistant\"),\n",
        "        (\"human\", \"{human_input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Initialize the OpenAI Chat instance with specific model parameters.\n",
        "chatgpt = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "# create simple llm chain\n",
        "llm_chain = (prompt\n",
        "                |\n",
        "             chatgpt\n",
        "                |\n",
        "             StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "XkY0BYNOdxoQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkY0BYNOdxoQ",
        "outputId": "48837443-20c2-4f04-8e95-ec68f2616abe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- AI, or artificial intelligence, refers to the simulation of human intelligence processes by machines, especially computer systems.\n",
            "- It involves the development of algorithms and models that enable machines to perform tasks that typically require human intelligence, such as learning, problem-solving, and decision-making.\n",
            "- AI technologies include machine learning, natural language processing, computer vision, and robotics, and are used in various applications across industries such as healthcare, finance, and transportation.\n"
          ]
        }
      ],
      "source": [
        "prompt_txt = \"Explain AI in 3 bullet points\"\n",
        "response = llm_chain.invoke({'human_input': prompt_txt})\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3t040jImhrGs",
      "metadata": {
        "id": "3t040jImhrGs"
      },
      "source": [
        "## Selective Tracing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "X4izzpOFhsnB",
      "metadata": {
        "id": "X4izzpOFhsnB"
      },
      "outputs": [],
      "source": [
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "uZjevIyliEPD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZjevIyliEPD",
        "outputId": "b7d8ca9a-d8fd-4c9f-85f0-e6384e893c42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generative AI is a type of artificial intelligence that can create new content, such as images, text, or music, based on patterns it has learned from existing data.\n"
          ]
        }
      ],
      "source": [
        "# does not get traced anymore\n",
        "prompt_txt = \"Explain Generative AI in one line\"\n",
        "response = llm_chain.invoke({'human_input': prompt_txt})\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4nnZW_vZhvch",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nnZW_vZhvch",
        "outputId": "1b8cbc25-743e-4d36-8e30-cc1613c608e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI, or artificial intelligence, refers to the simulation of human intelligence processes by machines, typically computer systems.\n"
          ]
        }
      ],
      "source": [
        "from langchain.callbacks.tracers import LangChainTracer\n",
        "\n",
        "# You can configure a LangChainTracer instance to trace a specific invocation.\n",
        "tracer = LangChainTracer()\n",
        "prompt_txt = \"Explain AI in one line\"\n",
        "response = llm_chain.invoke({'human_input': prompt_txt}, config={\"callbacks\": [tracer]})\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "_yZCw7_DiNwu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yZCw7_DiNwu",
        "outputId": "35274123-0c0d-4483-dfbd-297d39134538"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generative AI is a type of artificial intelligence that can create new content, such as images, text, or music, based on patterns and data it has been trained on.\n"
          ]
        }
      ],
      "source": [
        "# LangChain also supports a context manager for tracing a specific block of code.\n",
        "from langchain_core.tracers.context import tracing_v2_enabled\n",
        "\n",
        "prompt_txt = \"Explain Generative AI in one line\"\n",
        "with tracing_v2_enabled():\n",
        "    response = llm_chain.invoke({'human_input': prompt_txt})\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vfOZhPZklJ4v",
      "metadata": {
        "id": "vfOZhPZklJ4v"
      },
      "source": [
        "## Log traces to specific projects dynamically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "AmoCvRASk8zb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmoCvRASk8zb",
        "outputId": "1b55a226-064a-439a-8142-74bb5821d029"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI, or artificial intelligence, refers to the simulation of human intelligence processes by machines, such as learning, reasoning, and problem-solving.\n"
          ]
        }
      ],
      "source": [
        "tracer = LangChainTracer(project_name='LLM App Project - 2')\n",
        "prompt_txt = \"Explain AI in one line\"\n",
        "response = llm_chain.invoke({'human_input': prompt_txt}, config={\"callbacks\": [tracer]})\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "vDwOeyWMlR5C",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDwOeyWMlR5C",
        "outputId": "47975aa3-392a-46ff-96a5-c082adce16fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generative AI is a type of artificial intelligence that can create new content, such as images, text, or music, based on patterns and data it has been trained on.\n"
          ]
        }
      ],
      "source": [
        "prompt_txt = \"Explain Generative AI in one line\"\n",
        "with tracing_v2_enabled(project_name='LLM App Project - 2'):\n",
        "    response = llm_chain.invoke({'human_input': prompt_txt})\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "s2k9nqJhmrVN",
      "metadata": {
        "id": "s2k9nqJhmrVN"
      },
      "source": [
        "## Adding metadata and tags in traces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "LWTICq-Glf1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWTICq-Glf1a",
        "outputId": "05ae25ce-326a-4597-8087-0efecb641be2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generative AI is a type of artificial intelligence that can create new content, such as images, text, or music, based on patterns it has learned from existing data.\n"
          ]
        }
      ],
      "source": [
        "prompt_txt = \"Explain Generative AI in one line\"\n",
        "with tracing_v2_enabled():\n",
        "    response = llm_chain.invoke({'human_input': prompt_txt},  {\"tags\": ['AI', 'Data Science'],\n",
        "                                                               \"metadata\": {\"user\": \"aks\", \"team\": \"data science\"}})\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "kHdVlPnVnDn7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHdVlPnVnDn7",
        "outputId": "a3f89ed7-6393-4f31-c238-ea87eef232f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The fastest animal on land is the cheetah, which can reach speeds of up to 60-70 miles per hour (96-112 km/h) in short bursts covering distances up to 500 meters. In the air, the peregrine falcon holds the title for the fastest animal, reaching speeds of over 240 miles per hour (386 km/h) when diving to catch prey. In the water, the sailfish is considered the fastest swimmer, reaching speeds of up to 68 miles per hour (110 km/h).\n"
          ]
        }
      ],
      "source": [
        "prompt_txt = \"Explain which is the fastest animal?\"\n",
        "with tracing_v2_enabled():\n",
        "    response = llm_chain.invoke({'human_input': prompt_txt},  {\"tags\": ['General Knowledge', 'Environment'],\n",
        "                                                               \"metadata\": {\"user\": \"bob\", \"team\": \"social science\"}})\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iHx2x6vCn3m4",
      "metadata": {
        "id": "iHx2x6vCn3m4"
      },
      "source": [
        "## Customize run names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "S03gu5jCnQOM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S03gu5jCnQOM",
        "outputId": "a88eb8be-31e6-4fe7-b410-22ee70a226bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep learning is a subset of machine learning that involves training artificial neural networks to learn and make decisions from data. These neural networks are composed of multiple layers of interconnected nodes, which allow them to learn complex patterns and relationships in the data. Deep learning has been particularly successful in tasks such as image and speech recognition, natural language processing, and playing games. It is a powerful tool for solving problems that involve large amounts of data and can often outperform traditional machine learning algorithms in terms of accuracy and performance.\n"
          ]
        }
      ],
      "source": [
        "prompt_txt = \"Explain what is Deep Learning?\"\n",
        "with tracing_v2_enabled():\n",
        "    response = llm_chain.invoke({'human_input': prompt_txt},  {\"tags\": ['AI', 'Data Science'],\n",
        "                                                               \"metadata\": {\"user\": \"dj\", \"team\": \"data science\"},\n",
        "                                                               \"run_name\": \"AKSRunMay2025_001\"},\n",
        "                                )\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J-qoEXzvY2pP",
      "metadata": {
        "id": "J-qoEXzvY2pP"
      },
      "source": [
        "# Evaluating and Monitoring LLM Apps with LangSmith\n",
        "\n",
        "Here we will create an evaluation dataset and then evaluate our LLM app using various metrics and see how we can monitor our application using LangSmith"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "DtbhML-J2ZQj",
      "metadata": {
        "id": "DtbhML-J2ZQj"
      },
      "outputs": [],
      "source": [
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "# you can change this based on specific apps and projects\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"LLM App Project - 1\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HNsgjYFqZCcX",
      "metadata": {
        "id": "HNsgjYFqZCcX"
      },
      "source": [
        "## Access LLM App runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "upkz8STDpayI",
      "metadata": {
        "id": "upkz8STDpayI"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "zfoW6h8BzJO8",
      "metadata": {
        "id": "zfoW6h8BzJO8"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Initialize a client\n",
        "client = Client(timeout_ms=3600000)\n",
        "\n",
        "todays_llm_runs = client.list_runs(\n",
        "    project_name=\"LLM App Project - 1\",\n",
        "    start_time=datetime.now() - timedelta(days=1), # can change or remove this to retrieve more runs\n",
        "    run_type=\"llm\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "58FixDTu11sZ",
      "metadata": {
        "id": "58FixDTu11sZ"
      },
      "outputs": [],
      "source": [
        "dataset = []\n",
        "for run in todays_llm_runs:\n",
        "    dataset.append((run.inputs, run.outputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "4IZTPf3x2IUD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IZTPf3x2IUD",
        "outputId": "e578abfc-3816-4e09-cf2f-485816bb1716"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'messages': [[{'id': ['langchain', 'schema', 'messages', 'SystemMessage'],\n",
              "     'kwargs': {'content': 'Act as a helpful AI Assistant', 'type': 'system'},\n",
              "     'lc': 1,\n",
              "     'type': 'constructor'},\n",
              "    {'id': ['langchain', 'schema', 'messages', 'HumanMessage'],\n",
              "     'kwargs': {'content': 'Explain what is Deep Learning?', 'type': 'human'},\n",
              "     'lc': 1,\n",
              "     'type': 'constructor'}]]},\n",
              " {'generations': [[{'generation_info': {'finish_reason': 'stop',\n",
              "      'logprobs': None},\n",
              "     'message': {'id': ['langchain', 'schema', 'messages', 'AIMessage'],\n",
              "      'kwargs': {'content': 'Deep learning is a subset of machine learning that involves training artificial neural networks to learn and make decisions from data. These neural networks are composed of multiple layers of interconnected nodes, which allow them to learn complex patterns and relationships in the data. Deep learning has been particularly successful in tasks such as image and speech recognition, natural language processing, and playing games. It is a powerful tool for solving problems that involve large amounts of data and can often outperform traditional machine learning algorithms in terms of accuracy and performance.',\n",
              "       'id': 'run-b8465538-26eb-4195-b5c4-e37b59fa0cbc-0',\n",
              "       'invalid_tool_calls': [],\n",
              "       'response_metadata': {'finish_reason': 'stop',\n",
              "        'logprobs': None,\n",
              "        'model_name': 'gpt-3.5-turbo',\n",
              "        'system_fingerprint': None,\n",
              "        'token_usage': {'completion_tokens': 100,\n",
              "         'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
              "          'audio_tokens': 0,\n",
              "          'reasoning_tokens': 0,\n",
              "          'rejected_prediction_tokens': 0},\n",
              "         'prompt_tokens': 24,\n",
              "         'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0},\n",
              "         'total_tokens': 124}},\n",
              "       'tool_calls': [],\n",
              "       'type': 'ai'},\n",
              "      'lc': 1,\n",
              "      'type': 'constructor'},\n",
              "     'text': 'Deep learning is a subset of machine learning that involves training artificial neural networks to learn and make decisions from data. These neural networks are composed of multiple layers of interconnected nodes, which allow them to learn complex patterns and relationships in the data. Deep learning has been particularly successful in tasks such as image and speech recognition, natural language processing, and playing games. It is a powerful tool for solving problems that involve large amounts of data and can often outperform traditional machine learning algorithms in terms of accuracy and performance.',\n",
              "     'type': 'ChatGeneration'}]],\n",
              "  'llm_output': {'model_name': 'gpt-3.5-turbo',\n",
              "   'system_fingerprint': None,\n",
              "   'token_usage': {'completion_tokens': 100,\n",
              "    'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
              "     'audio_tokens': 0,\n",
              "     'reasoning_tokens': 0,\n",
              "     'rejected_prediction_tokens': 0},\n",
              "    'prompt_tokens': 24,\n",
              "    'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0},\n",
              "    'total_tokens': 124}},\n",
              "  'run': None})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input['messages'][0][1]['kwargs']['content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tLBrItckk9kc",
        "outputId": "4fd143ba-d89f-4cdb-8c0d-6b1b0ccd0701"
      },
      "id": "tLBrItckk9kc",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Explain AI in 3 bullet points'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CLGIo_qSZLoX",
      "metadata": {
        "id": "CLGIo_qSZLoX"
      },
      "source": [
        "## Create an evaluation dataset of inputs and outputs\n",
        "\n",
        "Ideally the outputs should be human-labeled or annotated outputs which are examples of ground-truth for input data. Here we will just use model outputs to quickly create an evaluation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "Iwn99wpl2SMz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iwn99wpl2SMz",
        "outputId": "36c8bab9-a6a5-4148-b5bf-5e1a0ffb9fdb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Explain AI in one line',\n",
              "  'AI, or artificial intelligence, refers to the simulation of human intelligence processes by machines, typically computer systems.'),\n",
              " ('Explain what is Deep Learning?',\n",
              "  'Deep learning is a subset of machine learning that involves training artificial neural networks to learn and make decisions from data. These neural networks are composed of multiple layers of interconnected nodes, which allow them to learn complex patterns and relationships in the data. Deep learning has been particularly successful in tasks such as image and speech recognition, natural language processing, and playing games. It is a powerful tool for solving problems that involve large amounts of data and can often outperform traditional machine learning algorithms in terms of accuracy and performance.'),\n",
              " ('Explain AI in 3 bullet points',\n",
              "  '- AI, or artificial intelligence, refers to the simulation of human intelligence processes by machines, especially computer systems.\\n- It involves the development of algorithms and models that enable machines to perform tasks that typically require human intelligence, such as learning, problem-solving, and decision-making.\\n- AI technologies include machine learning, natural language processing, computer vision, and robotics, and are used in various applications across industries such as healthcare, finance, and transportation.'),\n",
              " ('Explain which is the fastest animal?',\n",
              "  'The fastest animal on land is the cheetah, which can reach speeds of up to 60-70 miles per hour (96-112 km/h) in short bursts covering distances up to 500 meters. In the air, the peregrine falcon holds the title for the fastest animal, reaching speeds of over 240 miles per hour (386 km/h) when diving to catch prey. In the water, the sailfish is considered the fastest swimmer, reaching speeds of up to 68 miles per hour (110 km/h).'),\n",
              " ('Explain Generative AI in one line',\n",
              "  'Generative AI is a type of artificial intelligence that can create new content, such as images, text, or music, based on patterns it has learned from existing data.'),\n",
              " ('Explain Generative AI in one line',\n",
              "  'Generative AI is a type of artificial intelligence that can create new content, such as images, text, or music, based on patterns and data it has been trained on.')]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "refined_dataset = []\n",
        "for input, output in dataset:\n",
        "    refined_dataset.append((input['messages'][0][1]['kwargs']['content'],\n",
        "                            output['generations'][0][0]['text']))\n",
        "refined_dataset = list(set(refined_dataset))\n",
        "refined_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bmAEDcHmZZ6U",
      "metadata": {
        "id": "bmAEDcHmZZ6U"
      },
      "source": [
        "Let's also add in some human-labeled input-output examples where the output is created by humans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "1OKC2qwv3N6U",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OKC2qwv3N6U",
        "outputId": "e50cca2e-36c3-4d21-ed5d-2365e32750e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Explain AI in one line',\n",
              "  'AI, or artificial intelligence, refers to the simulation of human intelligence processes by machines, typically computer systems.'),\n",
              " ('Explain what is Deep Learning?',\n",
              "  'Deep learning is a subset of machine learning that involves training artificial neural networks to learn and make decisions from data. These neural networks are composed of multiple layers of interconnected nodes, which allow them to learn complex patterns and relationships in the data. Deep learning has been particularly successful in tasks such as image and speech recognition, natural language processing, and playing games. It is a powerful tool for solving problems that involve large amounts of data and can often outperform traditional machine learning algorithms in terms of accuracy and performance.'),\n",
              " ('Explain AI in 3 bullet points',\n",
              "  '- AI, or artificial intelligence, refers to the simulation of human intelligence processes by machines, especially computer systems.\\n- It involves the development of algorithms and models that enable machines to perform tasks that typically require human intelligence, such as learning, problem-solving, and decision-making.\\n- AI technologies include machine learning, natural language processing, computer vision, and robotics, and are used in various applications across industries such as healthcare, finance, and transportation.'),\n",
              " ('Explain which is the fastest animal?',\n",
              "  'The fastest animal on land is the cheetah, which can reach speeds of up to 60-70 miles per hour (96-112 km/h) in short bursts covering distances up to 500 meters. In the air, the peregrine falcon holds the title for the fastest animal, reaching speeds of over 240 miles per hour (386 km/h) when diving to catch prey. In the water, the sailfish is considered the fastest swimmer, reaching speeds of up to 68 miles per hour (110 km/h).'),\n",
              " ('Explain Generative AI in one line',\n",
              "  'Generative AI is a type of artificial intelligence that can create new content, such as images, text, or music, based on patterns it has learned from existing data.'),\n",
              " ('Explain Generative AI in one line',\n",
              "  'Generative AI is a type of artificial intelligence that can create new content, such as images, text, or music, based on patterns and data it has been trained on.'),\n",
              " ('What is the largest mammal?', 'The blue whale'),\n",
              " ('What do mammals and birds have in common?',\n",
              "  'Both are homeothermic (warm-blooded) animals'),\n",
              " (\"What's the main characteristic of amphibians?\",\n",
              "  'They live both in water and on land')]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "more_examples = [\n",
        "  (\"What is the largest mammal?\", \"The blue whale\"),\n",
        "  (\"What do mammals and birds have in common?\", \"Both are homeothermic (warm-blooded) animals\"),\n",
        "  (\"What's the main characteristic of amphibians?\", \"They live both in water and on land\"),\n",
        "]\n",
        "\n",
        "for input, output in more_examples:\n",
        "    refined_dataset.append((input, output))\n",
        "\n",
        "refined_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JNKg00MUZsi1",
      "metadata": {
        "id": "JNKg00MUZsi1"
      },
      "source": [
        "## Create an Evaluation Dataset in LangSmith\n",
        "\n",
        "Here we will upload our dataset to LangSmith cloud and create our eval dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "bKk3wnbLD3VX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKk3wnbLD3VX",
        "outputId": "70958425-c66b-4627-fb5f-88980d6fa02e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset(name='Sample LLM App Eval Dataset - AKS-Test001', description='Dataset of sample prompts and human outputs', data_type=<DataType.kv: 'kv'>, id=UUID('0b6a7d51-dd8e-4f70-93cb-104894a3e1c1'), created_at=datetime.datetime(2025, 5, 30, 14, 36, 41, 744930, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2025, 5, 30, 14, 36, 41, 744930, tzinfo=datetime.timezone.utc), example_count=0, session_count=0, last_session_start_time=None)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "# Initialize a client\n",
        "client = Client(timeout_ms=3600000)\n",
        "\n",
        "# Storing inputs in a dataset lets us\n",
        "# run chains and LLMs over a shared set of examples.\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name='Sample LLM App Eval Dataset - AKS-Test001',\n",
        "    description=\"Dataset of sample prompts and human outputs\",\n",
        ")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "MXajI_UISfKh",
      "metadata": {
        "id": "MXajI_UISfKh"
      },
      "outputs": [],
      "source": [
        "for input_prompt, output_answer in refined_dataset:\n",
        "    client.create_example(\n",
        "        inputs={\"question\": input_prompt},\n",
        "        outputs={\"answer\": output_answer},\n",
        "        metadata={\"source\": \"Wikipedia\"},\n",
        "        dataset_id=dataset.id,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "ADqQpXjbj9Ml",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADqQpXjbj9Ml",
        "outputId": "2682b0e5-7973-4b58-b435-3d35db8fb997"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Dataset(name='Sample LLM App Eval Dataset - AKS-Test001', description='Dataset of sample prompts and human outputs', data_type=<DataType.kv: 'kv'>, id=UUID('0b6a7d51-dd8e-4f70-93cb-104894a3e1c1'), created_at=datetime.datetime(2025, 5, 30, 14, 36, 41, 744930, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2025, 5, 30, 14, 36, 41, 744930, tzinfo=datetime.timezone.utc), example_count=9, session_count=0, last_session_start_time=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "datasets = client.list_datasets()\n",
        "list(datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DcQjcsBbZ_2U",
      "metadata": {
        "id": "DcQjcsBbZ_2U"
      },
      "source": [
        "## View evaluation dataset examples\n",
        "\n",
        "You can get examples of data points from your eval dataset in the cloud anytime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "jhFpjE7LkK7h",
      "metadata": {
        "id": "jhFpjE7LkK7h"
      },
      "outputs": [],
      "source": [
        "examples = client.list_examples(dataset_name=\"Sample LLM App Eval Dataset - AKS-Test001\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "kQGKxEvekbD6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQGKxEvekbD6",
        "outputId": "eb1fbb54-113b-4e05-bb61-0b2363c6d15e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset_id=UUID('0b6a7d51-dd8e-4f70-93cb-104894a3e1c1') inputs={'question': \"What's the main characteristic of amphibians?\"} outputs={'answer': 'They live both in water and on land'} metadata={'source': 'Wikipedia', 'dataset_split': ['base']} id=UUID('48e6482f-5a1d-4d33-918d-28ba34c9cf9f') created_at=datetime.datetime(2025, 5, 30, 14, 36, 52, 866047, tzinfo=datetime.timezone.utc) modified_at=datetime.datetime(2025, 5, 30, 14, 36, 52, 866047, tzinfo=datetime.timezone.utc) runs=[] source_run_id=None\n",
            "dataset_id=UUID('0b6a7d51-dd8e-4f70-93cb-104894a3e1c1') inputs={'question': 'What do mammals and birds have in common?'} outputs={'answer': 'Both are homeothermic (warm-blooded) animals'} metadata={'source': 'Wikipedia', 'dataset_split': ['base']} id=UUID('047b8988-80bf-43cf-9afd-9e119747da50') created_at=datetime.datetime(2025, 5, 30, 14, 36, 52, 607900, tzinfo=datetime.timezone.utc) modified_at=datetime.datetime(2025, 5, 30, 14, 36, 52, 607900, tzinfo=datetime.timezone.utc) runs=[] source_run_id=None\n",
            "dataset_id=UUID('0b6a7d51-dd8e-4f70-93cb-104894a3e1c1') inputs={'question': 'What is the largest mammal?'} outputs={'answer': 'The blue whale'} metadata={'source': 'Wikipedia', 'dataset_split': ['base']} id=UUID('44b8582f-65c3-46ca-a77d-b6f5e522524d') created_at=datetime.datetime(2025, 5, 30, 14, 36, 52, 65332, tzinfo=datetime.timezone.utc) modified_at=datetime.datetime(2025, 5, 30, 14, 36, 52, 65332, tzinfo=datetime.timezone.utc) runs=[] source_run_id=None\n",
            "dataset_id=UUID('0b6a7d51-dd8e-4f70-93cb-104894a3e1c1') inputs={'question': 'Explain Generative AI in one line'} outputs={'answer': 'Generative AI is a type of artificial intelligence that can create new content, such as images, text, or music, based on patterns and data it has been trained on.'} metadata={'source': 'Wikipedia', 'dataset_split': ['base']} id=UUID('c7309e3e-3904-4852-a325-9b4724a1bfc0') created_at=datetime.datetime(2025, 5, 30, 14, 36, 51, 816391, tzinfo=datetime.timezone.utc) modified_at=datetime.datetime(2025, 5, 30, 14, 36, 51, 816391, tzinfo=datetime.timezone.utc) runs=[] source_run_id=None\n",
            "dataset_id=UUID('0b6a7d51-dd8e-4f70-93cb-104894a3e1c1') inputs={'question': 'Explain Generative AI in one line'} outputs={'answer': 'Generative AI is a type of artificial intelligence that can create new content, such as images, text, or music, based on patterns it has learned from existing data.'} metadata={'source': 'Wikipedia', 'dataset_split': ['base']} id=UUID('b17074c3-2796-4d6c-9559-975214f26bdd') created_at=datetime.datetime(2025, 5, 30, 14, 36, 51, 586209, tzinfo=datetime.timezone.utc) modified_at=datetime.datetime(2025, 5, 30, 14, 36, 51, 586209, tzinfo=datetime.timezone.utc) runs=[] source_run_id=None\n",
            "dataset_id=UUID('0b6a7d51-dd8e-4f70-93cb-104894a3e1c1') inputs={'question': 'Explain which is the fastest animal?'} outputs={'answer': 'The fastest animal on land is the cheetah, which can reach speeds of up to 60-70 miles per hour (96-112 km/h) in short bursts covering distances up to 500 meters. In the air, the peregrine falcon holds the title for the fastest animal, reaching speeds of over 240 miles per hour (386 km/h) when diving to catch prey. In the water, the sailfish is considered the fastest swimmer, reaching speeds of up to 68 miles per hour (110 km/h).'} metadata={'source': 'Wikipedia', 'dataset_split': ['base']} id=UUID('f802276d-23d2-44ec-bf83-67abc951a5ea') created_at=datetime.datetime(2025, 5, 30, 14, 36, 51, 27367, tzinfo=datetime.timezone.utc) modified_at=datetime.datetime(2025, 5, 30, 14, 36, 51, 27367, tzinfo=datetime.timezone.utc) runs=[] source_run_id=None\n",
            "dataset_id=UUID('0b6a7d51-dd8e-4f70-93cb-104894a3e1c1') inputs={'question': 'Explain AI in 3 bullet points'} outputs={'answer': '- AI, or artificial intelligence, refers to the simulation of human intelligence processes by machines, especially computer systems.\\n- It involves the development of algorithms and models that enable machines to perform tasks that typically require human intelligence, such as learning, problem-solving, and decision-making.\\n- AI technologies include machine learning, natural language processing, computer vision, and robotics, and are used in various applications across industries such as healthcare, finance, and transportation.'} metadata={'source': 'Wikipedia', 'dataset_split': ['base']} id=UUID('3546bb06-1407-4e05-b4c2-24f0eec34565') created_at=datetime.datetime(2025, 5, 30, 14, 36, 50, 767903, tzinfo=datetime.timezone.utc) modified_at=datetime.datetime(2025, 5, 30, 14, 36, 50, 767903, tzinfo=datetime.timezone.utc) runs=[] source_run_id=None\n",
            "dataset_id=UUID('0b6a7d51-dd8e-4f70-93cb-104894a3e1c1') inputs={'question': 'Explain what is Deep Learning?'} outputs={'answer': 'Deep learning is a subset of machine learning that involves training artificial neural networks to learn and make decisions from data. These neural networks are composed of multiple layers of interconnected nodes, which allow them to learn complex patterns and relationships in the data. Deep learning has been particularly successful in tasks such as image and speech recognition, natural language processing, and playing games. It is a powerful tool for solving problems that involve large amounts of data and can often outperform traditional machine learning algorithms in terms of accuracy and performance.'} metadata={'source': 'Wikipedia', 'dataset_split': ['base']} id=UUID('4c6e44dd-424e-4760-b82f-066f9bf54ac9') created_at=datetime.datetime(2025, 5, 30, 14, 36, 50, 421731, tzinfo=datetime.timezone.utc) modified_at=datetime.datetime(2025, 5, 30, 14, 36, 50, 421731, tzinfo=datetime.timezone.utc) runs=[] source_run_id=None\n",
            "dataset_id=UUID('0b6a7d51-dd8e-4f70-93cb-104894a3e1c1') inputs={'question': 'Explain AI in one line'} outputs={'answer': 'AI, or artificial intelligence, refers to the simulation of human intelligence processes by machines, typically computer systems.'} metadata={'source': 'Wikipedia', 'dataset_split': ['base']} id=UUID('c6f42c65-9eb6-4cc9-976d-1e7c1adcc1c6') created_at=datetime.datetime(2025, 5, 30, 14, 36, 50, 184977, tzinfo=datetime.timezone.utc) modified_at=datetime.datetime(2025, 5, 30, 14, 36, 50, 184977, tzinfo=datetime.timezone.utc) runs=[] source_run_id=None\n"
          ]
        }
      ],
      "source": [
        "for example in examples:\n",
        "    print(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2yvLZ230a2Rt",
      "metadata": {
        "id": "2yvLZ230a2Rt"
      },
      "source": [
        "## Evaluate and Monitor LLM App performance\n",
        "\n",
        "We will leverage various evaluation metrics from LangSmith to test our LLM App performance here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "I6Tic61Pkb85",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "f7210ec2656c42dea64273efe39b8c48",
            "34baa6c62ab240c29eb39d1a9e7060ed",
            "6654a065031d47cfadff8e7937e951ed",
            "8712f759e8f44e65a5340501c7396591",
            "1175a7435f7948d9b8fab692c3c56c54",
            "e78382d44bf44226aeb27c45c5828c9e",
            "f4cbd668ffcc412c904c8186bd19b926",
            "33e09996e93347bc992288aa0029fdff",
            "3f26d31ed0144704aa167f1b1b58f7c0",
            "80c9156e5bff492091c23ddceda90a1d",
            "e980479f56f24d838d83ef9aa22f5c78"
          ]
        },
        "id": "I6Tic61Pkb85",
        "outputId": "79ee34e7-eed1-406b-c1db-ea4341e02ee3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for experiment: 'test_eval001-18041caf' at:\n",
            "https://smith.langchain.com/o/baa1c525-92bb-4e5c-9122-f20839cde3b8/datasets/0b6a7d51-dd8e-4f70-93cb-104894a3e1c1/compare?selectedSessions=6575b7b7-4f64-4255-a61c-7f4203096007\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f7210ec2656c42dea64273efe39b8c48"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from langsmith.evaluation import evaluate\n",
        "\n",
        "# Initialize a client\n",
        "client = Client(timeout_ms=3600000)\n",
        "\n",
        "results = evaluate(\n",
        "    lambda x: llm_chain.invoke({'human_input' : x['question']}),\n",
        "    client=client,\n",
        "    data=\"Sample LLM App Eval Dataset - AKS-Test001\",\n",
        "    experiment_prefix=\"test_eval001\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "WVHC-LqomTO1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "817b0961b08a4370822e338c0da7055e",
            "ba745070ef7745d1afda95d8036bf696",
            "b113846eb7d147bb97075a51c7f3306f",
            "da0d164fa0a8428c96c644292e4c972f",
            "95dda1b81b044ed39151e62f00a09117",
            "d342b3fe93c945409a328a5c55d292db",
            "a380b467b25f40c89fa49bf3bba3d6dc",
            "838f393038aa4db9bd626ec741695068",
            "c130de7bc33c43ada427a77b3a8c5d37",
            "ddaf510ebc124be19823d1e251d4da6e",
            "d7ff4d3ace524cde984d384c2493977f"
          ]
        },
        "id": "WVHC-LqomTO1",
        "outputId": "ad5bea36-f408-4e0f-d2e5-83e3ba827c90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for experiment: 'test_eval001-09786a8a' at:\n",
            "https://smith.langchain.com/o/baa1c525-92bb-4e5c-9122-f20839cde3b8/datasets/0b6a7d51-dd8e-4f70-93cb-104894a3e1c1/compare?selectedSessions=003477a8-d144-427d-85e9-bb9a3f5c909d\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "817b0961b08a4370822e338c0da7055e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
        "\n",
        "qa_evaluator = LangChainStringEvaluator(\"qa\")\n",
        "\n",
        "# Initialize a client\n",
        "client = Client(timeout_ms=3600000)\n",
        "\n",
        "results = evaluate(\n",
        "    lambda x: llm_chain.invoke({'human_input' : x['question']}),\n",
        "    client=client,\n",
        "    data=\"Sample LLM App Eval Dataset - AKS-Test001\",\n",
        "    experiment_prefix=\"test_eval001\",\n",
        "    evaluators=[qa_evaluator]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "W5Mj-tXCstlC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "b3caa3ec79584d8399d99755665c5c90",
            "4cb3355b14454d7dbf823f123e8d7778",
            "d138faf2f88f4ca7964e9255c221a8e4",
            "245aa6ace5a842b7981fffbbe6c18b1d",
            "bb89b37b3d074bee86abbc1b2ef5c579",
            "6029988977924b97bb767a82cf44a301",
            "ce892b0ee0014ed792a43079ff9bd2ac",
            "0fe15efbacc449f7b3bd856e2ee0823d",
            "2f025e77ff5543b69547db3898ddacc3",
            "d8e2d5d85c3f47f98db2ed5d7f860f0d",
            "cebefc6192e74fcb9e0d65a17c726305"
          ]
        },
        "id": "W5Mj-tXCstlC",
        "outputId": "c21ba740-a210-4892-d59e-1164be6b1025"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for experiment: 'test_eval001-3c755b93' at:\n",
            "https://smith.langchain.com/o/baa1c525-92bb-4e5c-9122-f20839cde3b8/datasets/0b6a7d51-dd8e-4f70-93cb-104894a3e1c1/compare?selectedSessions=2b2471e1-972d-42dd-ad33-21b351539779\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b3caa3ec79584d8399d99755665c5c90"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# from langchain.evaluation import EmbeddingDistance, load_evaluator\n",
        "# semantic_evaluator = load_evaluator(\n",
        "#     \"embedding_distance\", distance_metric=EmbeddingDistance.COSINE\n",
        "# )\n",
        "correct_evaluator = LangChainStringEvaluator(\"labeled_criteria\",\n",
        "                                             config={ \"criteria\": \"correctness\"})\n",
        "conciseness_evaluator =LangChainStringEvaluator(\"criteria\",\n",
        "                                                config={ \"criteria\": \"conciseness\"})\n",
        "helpfulness_evaluator = LangChainStringEvaluator(\"criteria\",\n",
        "                                                 config={ \"criteria\": \"helpfulness\"})\n",
        "semantic_evaluator = LangChainStringEvaluator(\"embedding_distance\")\n",
        "\n",
        "# Initialize a client\n",
        "client = Client(timeout_ms=3600000)\n",
        "\n",
        "results = evaluate(\n",
        "    lambda x: llm_chain.invoke({'human_input' : x['question']}),\n",
        "    client=client,\n",
        "    data=\"Sample LLM App Eval Dataset - AKS-Test001\",\n",
        "    experiment_prefix=\"test_eval001\",\n",
        "    evaluators=[correct_evaluator, conciseness_evaluator, helpfulness_evaluator]\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f7210ec2656c42dea64273efe39b8c48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_34baa6c62ab240c29eb39d1a9e7060ed",
              "IPY_MODEL_6654a065031d47cfadff8e7937e951ed",
              "IPY_MODEL_8712f759e8f44e65a5340501c7396591"
            ],
            "layout": "IPY_MODEL_1175a7435f7948d9b8fab692c3c56c54"
          }
        },
        "34baa6c62ab240c29eb39d1a9e7060ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e78382d44bf44226aeb27c45c5828c9e",
            "placeholder": "​",
            "style": "IPY_MODEL_f4cbd668ffcc412c904c8186bd19b926",
            "value": ""
          }
        },
        "6654a065031d47cfadff8e7937e951ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33e09996e93347bc992288aa0029fdff",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3f26d31ed0144704aa167f1b1b58f7c0",
            "value": 1
          }
        },
        "8712f759e8f44e65a5340501c7396591": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80c9156e5bff492091c23ddceda90a1d",
            "placeholder": "​",
            "style": "IPY_MODEL_e980479f56f24d838d83ef9aa22f5c78",
            "value": " 9/? [00:05&lt;00:00,  5.10s/it]"
          }
        },
        "1175a7435f7948d9b8fab692c3c56c54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e78382d44bf44226aeb27c45c5828c9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4cbd668ffcc412c904c8186bd19b926": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33e09996e93347bc992288aa0029fdff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "3f26d31ed0144704aa167f1b1b58f7c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "80c9156e5bff492091c23ddceda90a1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e980479f56f24d838d83ef9aa22f5c78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "817b0961b08a4370822e338c0da7055e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ba745070ef7745d1afda95d8036bf696",
              "IPY_MODEL_b113846eb7d147bb97075a51c7f3306f",
              "IPY_MODEL_da0d164fa0a8428c96c644292e4c972f"
            ],
            "layout": "IPY_MODEL_95dda1b81b044ed39151e62f00a09117"
          }
        },
        "ba745070ef7745d1afda95d8036bf696": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d342b3fe93c945409a328a5c55d292db",
            "placeholder": "​",
            "style": "IPY_MODEL_a380b467b25f40c89fa49bf3bba3d6dc",
            "value": ""
          }
        },
        "b113846eb7d147bb97075a51c7f3306f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_838f393038aa4db9bd626ec741695068",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c130de7bc33c43ada427a77b3a8c5d37",
            "value": 1
          }
        },
        "da0d164fa0a8428c96c644292e4c972f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddaf510ebc124be19823d1e251d4da6e",
            "placeholder": "​",
            "style": "IPY_MODEL_d7ff4d3ace524cde984d384c2493977f",
            "value": " 9/? [00:06&lt;00:00,  3.04it/s]"
          }
        },
        "95dda1b81b044ed39151e62f00a09117": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d342b3fe93c945409a328a5c55d292db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a380b467b25f40c89fa49bf3bba3d6dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "838f393038aa4db9bd626ec741695068": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "c130de7bc33c43ada427a77b3a8c5d37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ddaf510ebc124be19823d1e251d4da6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7ff4d3ace524cde984d384c2493977f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b3caa3ec79584d8399d99755665c5c90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4cb3355b14454d7dbf823f123e8d7778",
              "IPY_MODEL_d138faf2f88f4ca7964e9255c221a8e4",
              "IPY_MODEL_245aa6ace5a842b7981fffbbe6c18b1d"
            ],
            "layout": "IPY_MODEL_bb89b37b3d074bee86abbc1b2ef5c579"
          }
        },
        "4cb3355b14454d7dbf823f123e8d7778": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6029988977924b97bb767a82cf44a301",
            "placeholder": "​",
            "style": "IPY_MODEL_ce892b0ee0014ed792a43079ff9bd2ac",
            "value": ""
          }
        },
        "d138faf2f88f4ca7964e9255c221a8e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fe15efbacc449f7b3bd856e2ee0823d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f025e77ff5543b69547db3898ddacc3",
            "value": 1
          }
        },
        "245aa6ace5a842b7981fffbbe6c18b1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8e2d5d85c3f47f98db2ed5d7f860f0d",
            "placeholder": "​",
            "style": "IPY_MODEL_cebefc6192e74fcb9e0d65a17c726305",
            "value": " 9/? [00:45&lt;00:00,  3.63s/it]"
          }
        },
        "bb89b37b3d074bee86abbc1b2ef5c579": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6029988977924b97bb767a82cf44a301": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce892b0ee0014ed792a43079ff9bd2ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0fe15efbacc449f7b3bd856e2ee0823d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "2f025e77ff5543b69547db3898ddacc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d8e2d5d85c3f47f98db2ed5d7f860f0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cebefc6192e74fcb9e0d65a17c726305": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}